<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>python爬虫入门</title>
      <link href="/2023/05/16/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"/>
      <url>/2023/05/16/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="爬虫的基础知识"><a href="#爬虫的基础知识" class="headerlink" title="爬虫的基础知识"></a>爬虫的基础知识</h1><h2 id="为什么要学习爬虫"><a href="#为什么要学习爬虫" class="headerlink" title="为什么要学习爬虫"></a>为什么要学习爬虫</h2><p>如今，人工智能，大数据离我们越来越近，很多公司在开展相关的业务，但是人工智能和大数据中有一个东西非常重要，那就是数据，但是数据从哪里来呢？</p><p>首先我们来看下面这个例子：</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E6%96%B0%E6%B5%AA%E6%8C%87%E6%95%B0.png"></p><p>这是微博的<a href="http://data.weibo.com/index">微指数</a>的一个截图，他把在微博上的用户的微博和评论中的关键词语做了提取，然后进行了统计，然后根据统计结果得出某个词语的流行趋势，之后进行了简单的展示</p><p>类似微指数的网站还有很多，比如百度指数，阿里指数，360指数等等，这些网站有非常大的用户量，他们能够获取自己用户的数据进行统计和分析</p><p>那么对于一些中小型的公司，没有如此大的用户量的时候，他们该怎么办呢？</p><h3 id="数据的来源"><a href="#数据的来源" class="headerlink" title="数据的来源"></a>数据的来源</h3><ul><li>去第三方的公司购买数据(比如企查查)</li><li>去免费的数据网站下载数据(比如国家统计局)</li><li>通过爬虫爬取数据</li><li>人工收集数据(比如问卷调查)</li></ul><p>在上面的来源中：人工的方式费时费力，免费的数据网站上的数据质量不佳，很多第三方的数据公司他们的数据来源往往也是爬虫获取的，所以获取数据最有效的途径就是通过爬虫爬取</p><h3 id="爬取到的数据用途"><a href="#爬取到的数据用途" class="headerlink" title="爬取到的数据用途"></a>爬取到的数据用途</h3><p><a href="http://news.baidu.com/">百度新闻</a>,一家并不是做新闻的公司，这个网站上的新闻数据从哪里来的呢？</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%99%BE%E5%BA%A6%E6%96%B0%E9%97%BB.png"></p><p>通过点击，我们可以发现，他的新闻数据都是其他网站上的，在百度新闻上仅仅做了展示</p><p>如果后续我们要做一个网站，天天新闻，是不是也可以这样做呢</p><p>那么同样的，我们后续想要做一个和<a href="http://music.163.com/#/discover/playlist">网易云音乐</a>类似的音乐网站，是不是也可以这样来做呢？</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90.png"></p><p>通过前面的列子，能够总结出，爬虫获取的数据的用途：</p><ul><li>进行在网页或者是app上进行展示</li><li>进行数据分析或者是机器学习相关的项目</li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li>数据的来源：<ul><li>去第三方的公司购买数据(比如企查查)</li><li>去免费的数据网站下载数据(比如国家统计局)</li><li>通过爬虫爬取数据</li><li>人工收集数据(比如问卷调查)</li></ul></li><li>爬虫的概念：模拟浏览器发送网络请求，接收请求响应</li></ol><h2 id="什么是爬虫"><a href="#什么是爬虫" class="headerlink" title="什么是爬虫"></a>什么是爬虫</h2><p>网络爬虫（又被称为网页蜘蛛，网络机器人）就是模拟浏览器发送网络请求，接收请求响应，一种按照一定的规则，自动地抓取互联网信息的程序。</p><p>原则上，只要是浏览器(客户端)能做的事情，爬虫都能够做</p><h3 id="爬虫的用途"><a href="#爬虫的用途" class="headerlink" title="爬虫的用途"></a>爬虫的用途</h3><ul><li>12306抢票</li><li>网站上的投票</li><li>收集数据</li><li>刷流量和秒杀</li><li>爬虫调研</li></ul><h3 id="爬虫的分类"><a href="#爬虫的分类" class="headerlink" title="爬虫的分类"></a>爬虫的分类</h3><p>根据被爬网站的数量的不同，我们把爬虫分为：</p><ul><li>通用爬虫 ：通常指搜索引擎的爬虫</li><li>聚焦爬虫 ：针对特定网站的爬虫</li></ul><h3 id="爬虫的流程"><a href="#爬虫的流程" class="headerlink" title="爬虫的流程"></a>爬虫的流程</h3><p>聚焦爬虫的流程图：</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E8%81%9A%E7%84%A6%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B%E5%9B%BE.png"></p><ul><li>确认目标的url（地址）</li><li>发送网络请求（模拟正常的用户），得到对应的响应数据</li><li>提取出特定的数据</li><li>将数据进行保存，可以保存到本地、入库（数据库）</li></ul><h3 id="robots协议"><a href="#robots协议" class="headerlink" title="robots协议"></a>robots协议</h3><p>Robots协议：网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取，但它仅仅是互联网中的一般约定</p><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><ol><li>爬虫分类：通用爬虫、聚焦爬虫</li><li>爬虫的流程：<ul><li>向起始url发送请求，并获取响应</li><li>对响应进行提取</li><li>如果提取url，则继续发送请求获取响应</li><li>如果提取数据，则将数据进行保存</li></ul></li><li>robots协议：无需遵守该协议</li></ol><h2 id="http和https的概念"><a href="#http和https的概念" class="headerlink" title="http和https的概念"></a>http和https的概念</h2><ul><li>HTTP<ul><li>超文本传输协议</li><li>默认端口号:80</li></ul></li><li>HTTPS<ul><li>HTTP + SSL(安全套接字层)，即带有安全套接字层的超本文传输协议</li><li>默认端口号：443</li></ul></li></ul><p>HTTPS比HTTP更安全，但是性能更低</p><h3 id="浏览器发送HTTP请求的过程"><a href="#浏览器发送HTTP请求的过程" class="headerlink" title="浏览器发送HTTP请求的过程"></a>浏览器发送HTTP请求的过程</h3><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/http%E5%8F%91%E9%80%81%E7%9A%84%E8%BF%87%E7%A8%8B.png"></p><h3 id="http请求的过程"><a href="#http请求的过程" class="headerlink" title="http请求的过程"></a>http请求的过程</h3><ol><li>浏览器先向地址栏中的url发起请求，并获取相应</li><li>在返回的响应内容（html）中，会带有css、js、图片等url地址，以及ajax代码，浏览器按照响应内容中的顺序依次发送其他的请求，并获取相应的响应</li><li>浏览器每获取一个响应就对展示出的结果进行添加（加载），js，css等内容会修改页面的内容，js也可以重新发送请求，获取响应</li><li>从获取第一个响应并在浏览器中展示，直到最终获取全部响应，并在展示的结果中添加内容或修改————这个过程叫做浏览器的<strong>渲染</strong></li></ol><p><strong>注意:</strong></p><p>但是在爬虫中，爬虫只会请求url地址，对应的拿到url地址对应的响应（该响应的内容可以是html，css，js，图片等）</p><p>浏览器渲染出来的页面和爬虫请求的页面很多时候并不一样</p><p><strong>所以在爬虫中，需要以url地址对应的响应为准来进行数据的提取</strong></p><h3 id="http请求的形式"><a href="#http请求的形式" class="headerlink" title="http请求的形式"></a>http请求的形式</h3><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/http%E7%9A%84%E8%AF%B7%E6%B1%82%E5%BD%A2%E5%BC%8F.png"></p><p>这个图大家见过很多次，那么在浏览器headers中，点击view source来具体观察其中的请求行，请求头部和请求数据是什么样子的</p><h3 id="HTTP常见请求头"><a href="#HTTP常见请求头" class="headerlink" title="HTTP常见请求头"></a>HTTP常见请求头</h3><ol><li>Host (主机和端口号)</li><li>Connection (链接类型)</li><li>Upgrade-Insecure-Requests (升级为HTTPS请求)</li><li>User-Agent (浏览器名称)</li><li>Accept (传输文件类型)</li><li>Referer (页面跳转处)</li><li>Accept-Encoding（文件编解码格式）</li><li>Cookie （Cookie）</li><li>x-requested-with :XMLHttpRequest (表示该请求是Ajax异步请求)</li></ol><h3 id="HTTP重要的响应头"><a href="#HTTP重要的响应头" class="headerlink" title="HTTP重要的响应头"></a>HTTP重要的响应头</h3><ol><li>Set-Cookie （对方服务器设置cookie到用户浏览器的缓存）</li></ol><h3 id="响应状态码-status-code"><a href="#响应状态码-status-code" class="headerlink" title="响应状态码(status code)"></a>响应状态码(status code)</h3><p>常见的状态码：</p><ul><li>200：成功</li><li>302：临时转移至新的url</li><li>307：临时转移至新的url</li><li>404：找不到该页面</li><li>500：服务器内部错误</li><li>503：服务不可用，一般是被反爬</li></ul><h3 id="如何查看客户端和服务端的交流过程"><a href="#如何查看客户端和服务端的交流过程" class="headerlink" title="如何查看客户端和服务端的交流过程"></a>如何查看客户端和服务端的交流过程</h3><p>这里以Google浏览器中百度首页为例进行介绍：</p><ol><li><p>鼠标右键点击网页，点击“检查”</p></li><li><p>找到Network点击，里面存有网络数据的信息</p></li><li><p>刷新页面，点击访问百度首页</p></li><li><p>发现在Network中有很多的数据组成了我们看到的网页（html、css、js、jpg）</p></li><li><p>寻找All里面就是全部的数据包</p></li><li><p>一般，整体的骨架数据包，就是第一个</p><p>①Headers：请求的信息request响应的信息response</p><p>②Preview：预览效果的样式（图片的缺少代表需要其他的数据包填充）</p><p>③Response：数据包的类型是html，里面就是html数据的源代码</p></li><li><p>General：整体的信息描述</p><p>①Request URL：此数据包的域名</p><p>②Request Method：请求方式GETPOST</p><p>③Status Code：状态码</p><p>④Remote Address：ip:端口号</p></li><li><p>Response Headers（了解即可）：响应头（响应信息）</p><p>服务器需要遵循这种规则协议，浏览器才能解析出来，并且展示</p></li><li><p>Request Headers<strong>（重点）</strong>：请求头（请求信息）</p><p>（浏览器、爬虫程序）向服务器发送请求，需要遵循http、https协议</p><p>①Accept：告诉服务器请求的客户端所能接收的响应类型</p><p>②Accept-Encoding：所支持的压缩算法类型</p><p>③Accept-Language：所使用的语言偏好，可用于响应内容本地化</p><p>④Cache-Control：指定在客户端和服务器之间缓存和重新验证缓存的行为</p><p>⑤Connection：指定了客户端与服务器之间的连接类型</p><p>⑥Cookie<strong>（重点）</strong>：用于在客户端和服务端之间传递和储存信息</p><p>一个完整的Cookie通常包括了以下信息：</p><ul><li>名称：唯一标识一个Cookie的名称</li><li>值：与名称相关联的信息</li><li>域名：设置Cookie生效的域名</li><li>路径：设置Cookie生效的路径</li><li>过期时间：设置Cookie的过期时间，过期后浏览器会自动删除Cookie</li><li>安全标识：表示浏览器只能通过HTTPS和SSL等安全协议加密传输该Cookie的信息，提高Cookie传输过程中的安全性。</li></ul><p>⑦Host：指定了被请求资源所在的主机名（域名或者IP地址）</p><p>⑧User-Agent：包含了发出请求的用户代理标识信息。通过User-Agent头，服务器可以识别客户端使用的操作系统、浏览器和版本等信息，从而可以向客户端返回合适的内容</p></li></ol><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><ol><li><p>记忆 http、https的概念和区别：</p><ul><li>http: 超本文传输协议</li><li>https: HTTP + SSL，即带有安全套接字层的超本文传输协议</li></ul></li><li><p>记忆 浏览器发送http请求的过程:</p><ul><li>浏览器先向地址栏中的url发起请求，并获取相应</li><li>在返回的响应内容（html）中，会带有css、js、图片等url地址，以及ajax代码，浏览器按照响应内容中的顺序依次发送其他的请求，并获取相应的响应</li><li>浏览器每获取一个响应就对展示出的结果进行添加（加载），js，css等内容会修改页面的内容，js也可以重新发送请求，获取响应</li><li>从获取第一个响应并在浏览器中展示，直到最终获取全部响应，并在展示的结果中添加内容或修改</li></ul></li><li><p>记忆 http请求头的形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GET /item/<span class="number">503</span>/<span class="number">1227315</span>?fr=aladdin HTTP/<span class="number">1.1</span></span><br><span class="line">Host: www.baidu.com</span><br><span class="line">......</span><br></pre></td></tr></table></figure></li><li><p>记忆 http响应头的形式 :</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HTTP/<span class="number">1.1</span> <span class="number">200</span> OK</span><br><span class="line">Connection: keep-alive</span><br><span class="line">......</span><br></pre></td></tr></table></figure></li><li><p>了解 http响应状态码</p><ul><li>200：成功</li><li>302：临时转移至新的url</li></ul></li></ol><h2 id="字符串相关复习"><a href="#字符串相关复习" class="headerlink" title="字符串相关复习"></a>字符串相关复习</h2><h3 id="字符和字符集"><a href="#字符和字符集" class="headerlink" title="字符和字符集"></a>字符和字符集</h3><p>字符(Character)是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等</p><p>字符集(Character set)是多个字符的集合</p><p>字符集包括：ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集等</p><p>ASCII编码是1个字节，而Unicode编码通常是2个字节。</p><p><strong>UTF-8是Unicode的实现方式</strong>之一，UTF-8是它是一种变长的编码方式，可以是1，2，3个字节</p><h3 id="python3中的字符串"><a href="#python3中的字符串" class="headerlink" title="python3中的字符串"></a>python3中的字符串</h3><p>python3中两种字符串类型：</p><ul><li>str : unicode的呈现形式</li><li>bytes :字节类型，互联网上数据的都是以二进制的方式(字节类型)传输的</li></ul><h3 id="str和bytes类型的互相转换"><a href="#str和bytes类型的互相转换" class="headerlink" title="str和bytes类型的互相转换"></a>str和bytes类型的互相转换</h3><ul><li>str 使用encode方法转化为 bytes</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">&#x27;abc&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(s))</span><br><span class="line"><span class="comment">#str编码变为bytes类型</span></span><br><span class="line">b = s.encode()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(b))</span><br></pre></td></tr></table></figure><ul><li>bytes 通过decode转化为 str</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b = <span class="string">b&#x27;abc&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(b))</span><br><span class="line"><span class="comment">#bytes类型解码称为str类型</span></span><br><span class="line">s = b.decode()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(s))</span><br></pre></td></tr></table></figure><p><strong>注意：编码方式解码方式必须一样，否则就会出现乱码</strong></p><h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><ol><li>str，bytes以及互相转换:<ul><li>str 使用encode方法转化为 bytes</li><li>bytes 通过decode转化为 str</li></ul></li><li>字符集编码类型（ASCII,unicode,UTF-8）:<ul><li>字符(Character)是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等</li><li>字符集(Character set)有字符构成，是多个字符的集合，如：ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集等</li></ul></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/1.1%E6%80%BB%E7%BB%93xmind.png"></p><h1 id="请求的发送方法"><a href="#请求的发送方法" class="headerlink" title="请求的发送方法"></a>请求的发送方法</h1><h2 id="requests模块的基本使用"><a href="#requests模块的基本使用" class="headerlink" title="requests模块的基本使用"></a>requests模块的基本使用</h2><p>​requests是一个常用的Python第三方库，用于发送HTTP请求，并处理HTTP响应。它可以发送GET、POST、PUT、DELETE等类型的HTTP请求，并支持Session会话和Cookie的自动管理，同时还支持HTTP代理、HTTPS证书验证和上传文件等常见操作。它的基本用法非常简单，只<strong>需要发送HTTP请求并获取响应</strong>，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure><h3 id="为什么要重点学习requests模块，而不是urllib"><a href="#为什么要重点学习requests模块，而不是urllib" class="headerlink" title="为什么要重点学习requests模块，而不是urllib"></a>为什么要重点学习requests模块，而不是urllib</h3><ul><li>requests的底层实现就是urllib</li><li>requests在python2 和python3中通用，方法完全一样</li><li>requests简单易用</li><li>Requests能够自动帮助我们解压(gzip压缩的等)响应内容</li></ul><h3 id="requests模块的安装"><a href="#requests模块的安装" class="headerlink" title="requests模块的安装"></a>requests模块的安装</h3><p>Windows中requests模块安装：</p><ul><li>win + r &gt;&gt; 输入cmd &gt;&gt; 回车输入：pip install requests&#x3D;&#x3D;2.24.0 -i <a href="https://pypi.doubanio.com/simple">https://pypi.doubanio.com/simple</a></li><li><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/requests%E6%A8%A1%E5%9D%97%E5%AE%89%E8%A3%85.png"></li></ul><p>Mac中requests模块安装：</p><ul><li>win + r &gt;&gt; 输入cmd &gt;&gt; 回车输入：pip3 install requests&#x3D;&#x3D;2.24.0 -i <a href="https://pypi.doubanio.com/simple">https://pypi.doubanio.com/simple</a></li><li>没有Mac电脑图片就不展示了</li></ul><h3 id="requests模块发送简单的get请求、获取响应"><a href="#requests模块发送简单的get请求、获取响应" class="headerlink" title="requests模块发送简单的get请求、获取响应"></a>requests模块发送简单的get请求、获取响应</h3><p>需求：通过requests向百度首页发送请求，获取百度首页的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment">#目标url</span></span><br><span class="line">url = <span class="string">&quot;https://www.baidu.com&quot;</span></span><br><span class="line"><span class="comment">#向目标url发送get请求</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="comment"># 打印响应内容</span></span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure><p><strong>response的常用属性：</strong></p><ul><li><code>response.text</code> 响应体 str类型</li><li><code>respones.content</code> 响应体 bytes类型</li><li><code>response.status_code</code> 响应状态码</li><li><code>response.request.headers</code> 响应对应的请求头</li><li><code>response.headers</code> 响应头</li><li><code>response.request.cookies</code> 响应对应请求的cookie</li><li><code>response.cookies</code> 响应的cookie（经过了set-cookie动作）</li></ul><p> <strong>response.text 和response.content的区别</strong></p><ul><li><p><code>response.text</code></p><ul><li>类型：str</li><li>解码类型： requests模块自动根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码</li><li>如何修改编码方式：<code>response.encoding=”gbk”</code></li></ul></li><li><p><code>response.content</code></p><ul><li>类型：bytes</li><li>解码类型： 没有指定</li><li>如何修改编码方式：<code>response.content.decode(“utf8”)</code></li></ul></li></ul><p>获取网页源码的通用方式：</p><ol><li><code>response.content.decode()</code></li><li><code>response.content.decode(&quot;GBK&quot;)</code></li><li><code>response.text</code></li></ol><p>以上三种方法从前往后尝试，能够100%的解决所有网页解码的问题</p><p>所以：更推荐使用<code>response.content.decode()</code>的方式获取响应的html页面</p><h3 id="小练习：把网络上的图片保存到本地"><a href="#小练习：把网络上的图片保存到本地" class="headerlink" title="小练习：把网络上的图片保存到本地"></a>小练习：把网络上的图片保存到本地</h3><blockquote><p>我们来把<a href="http://www.baidu.com的logo图片保存到本地/">www.baidu.com的logo图片保存到本地</a></p></blockquote><p><strong>思考：</strong></p><ul><li>以什么方式打开文件</li><li>保存什么格式的内容</li></ul><p><strong>分析：</strong></p><ul><li>图片的url：<a href="https://www.baidu.com/img/flexible/logo/pc/index.png">https://www.baidu.com/img/flexible/logo/pc/index.png</a></li><li>利用requests模块发送请求获取响应</li><li>以二进制写入的方式打开文件，并将response响应的二进制内容写入</li></ul><p><strong>代码实现：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment">#图片的url</span></span><br><span class="line">url_ = <span class="string">&#x27;https://www.baidu.com/img/flexible/logo/pc/index.png&#x27;</span></span><br><span class="line"><span class="comment">#响应本身就是一个图片，并且是二进制类型</span></span><br><span class="line">response = requests.get(url_)</span><br><span class="line"><span class="comment">#以二进制+写入的方式打开文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;baidu.png&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content)</span><br></pre></td></tr></table></figure><h3 id="发送带header的请求"><a href="#发送带header的请求" class="headerlink" title="发送带header的请求"></a>发送带header的请求</h3><blockquote><p>我们先写一个获取百度首页的代码</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># 目标url</span></span><br><span class="line">url_ = <span class="string">&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line"><span class="comment"># 发送请求</span></span><br><span class="line">response = requests.get(url_)</span><br><span class="line"><span class="comment"># 打印响应对应的请求</span></span><br><span class="line"><span class="comment"># print(response.content)</span></span><br><span class="line"><span class="comment"># 打印响应对应请求的请求头信息</span></span><br><span class="line"><span class="built_in">print</span>(response.request.headers)</span><br></pre></td></tr></table></figure><p><strong>1.思考</strong></p><p>对比浏览器上百度首页的网页源码和代码中的百度首页的源码，有什么不同？</p><p>代码中的百度首页的源码非常少，为什么？</p><p><strong>2.为什么请求需要带上header？</strong></p><p>模拟浏览器，欺骗服务器，获取和浏览器一致的内容</p><p><strong>3.header的形式：字典</strong></p><p>headers &#x3D; {“User-Agent”: “Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;113.0.0.0 Safari&#x2F;537.36”}</p><p> <strong>4.用法</strong></p><p>requests.get(url, headers&#x3D;headers)</p><p><strong>完整的代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># 目标url</span></span><br><span class="line">url_ = <span class="string">&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line"><span class="comment"># 进行伪装</span></span><br><span class="line">headers = &#123;<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&quot;</span>&#125;</span><br><span class="line"><span class="comment"># 在请求头中带上User-Agent，模拟浏览器发送请求</span></span><br><span class="line">response = requests.get(url_,headers=headers)</span><br><span class="line"><span class="comment"># 打印响应对应的请求</span></span><br><span class="line"><span class="comment"># print(response.content)</span></span><br><span class="line"><span class="comment"># 打印响应对应请求的头信息</span></span><br><span class="line"><span class="built_in">print</span>(response.request.headers)</span><br></pre></td></tr></table></figure><h3 id="发送带参数的请求"><a href="#发送带参数的请求" class="headerlink" title="发送带参数的请求"></a>发送带参数的请求</h3><blockquote><p>我们在使用百度搜索的时候经常发现url地址中会有一个 <code>?</code>，那么该问号后边的就是请求参数，又叫做查询字符串</p></blockquote><p><strong>1.什么叫做请求参数</strong></p><p>例1： <a href="http://www.webkaka.com/tutorial/server/2015/021013/">http://www.webkaka.com/tutorial/server/2015/021013/</a></p><p>例2：<a href="https://www.baidu.com/s?wd=python&a=c">https://www.baidu.com/s?wd=python&amp;a=c</a></p><p>例1中没有请求参数！例2中?后边的就是请求参数</p><p><strong>2.请求参数的形式：字典</strong></p><p>kw &#x3D; {‘wd’:’长城’}</p><p><strong>3.请求参数的用法</strong></p><p>requests.get(url,params&#x3D;kw)</p><p><strong>4.关于参数的注意点</strong></p><p>在url地址中，很多参数是没有用的，比如百度搜索的url地址，其中参数只有一个字段有用，其他的都可以删除，如果确定那些请求参数有用或者没用：挨个尝试，对应的！在后续的爬虫中，遇到很多参数的url地址，都可以尝试删除参数</p><p><strong>两种方式：发送带参数的请求</strong></p><blockquote><p>对<code>https://www.baidu.com/s?wd=python</code>发起请求可以使用<code>requests.get(url, params=kw)</code>的方式</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式一：利用params参数发送带参数的请求</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># 进行伪装</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 这是目标url</span></span><br><span class="line"><span class="comment"># url_ = &#x27;https://www.baidu.com/s?&#x27;wd=python&#x27;</span></span><br><span class="line"><span class="comment"># 最后有没有问好结果都一样</span></span><br><span class="line">url_ = <span class="string">&#x27;https://www.baidu.com/s?&#x27;</span></span><br><span class="line"><span class="comment"># 请求参数是一个字典，即wd=python</span></span><br><span class="line">kw = &#123;<span class="string">&#x27;wd&#x27;</span>:<span class="string">&#x27;python&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 带上请求参数发起请求，获取响应</span></span><br><span class="line">response = requests.get(url_,headers,params=kw)</span><br><span class="line"><span class="comment"># 当有多个请求参数时，requests接收的params参数为多个键值对的字典，比如&#x27;?wd=python&amp;a=c&#x27;--&gt;&#123;&#x27;wd&#x27;:&#x27;python&#x27;,&#x27;a&#x27;:&#x27;c&#x27;&#125;</span></span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br></pre></td></tr></table></figure><blockquote><p>也可以直接对<code>https://www.baidu.com/s?wd=python</code>完整的url直接发送请求，不使用params参数</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式二：直接发送带参数的url请求</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&#x27;</span>&#125;</span><br><span class="line">url_ = <span class="string">&#x27;https://www.baidu.com/s?wd=python&#x27;</span></span><br><span class="line"><span class="comment"># kw = &#123;&#x27;wd&#x27;:&#x27;python&#x27;&#125;</span></span><br><span class="line"><span class="comment">#url中包含了请求参数，所以此时无需params</span></span><br><span class="line">response = requests.get(url_,headers=headers)</span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br></pre></td></tr></table></figure><h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><ol><li>requests模块的介绍：能够帮助我们发起请求获取响应</li><li>requests的基本使用：<code>requests.get(url)</code></li><li>以及response常见的属性：<ul><li><code>response.text</code> 响应体 str类型</li><li><code>respones.content</code> 响应体 bytes类型</li><li><code>response.status_code</code> 响应状态码</li><li><code>response.request.headers</code> 响应对应的请求头</li><li><code>response.headers</code> 响应头</li><li><code>response.request._cookies</code> 响应对应请求的cookie</li><li><code>response.cookies</code> 响应的cookie（经过了set-cookie动作）</li></ul></li><li>掌握 requests.text和content的区别：text返回str类型，content返回bytes类型</li><li>掌握 解决网页的解码问题：<ul><li><code>response.content.decode()</code></li><li><code>response.content.decode(&quot;GBK&quot;)</code></li><li><code>response.text</code></li></ul></li><li>掌握 requests模块发送带headers的请求：<code>requests.get(url, headers=&#123;&#125;)</code></li><li>掌握 requests模块发送带参数的get请求：<code>requests.get(url, params=&#123;&#125;)</code></li></ol><h2 id="requests模块的深入使用"><a href="#requests模块的深入使用" class="headerlink" title="requests模块的深入使用"></a>requests模块的深入使用</h2><h3 id="使用requests发送POST请求"><a href="#使用requests发送POST请求" class="headerlink" title="使用requests发送POST请求"></a>使用requests发送POST请求</h3><blockquote><p>思考：哪些地方我们会用到POST请求？</p></blockquote><ol><li>登录注册（ POST 比 GET 更安全）</li><li>需要传输大文本内容的时候（ POST 请求对数据长度没有要求）</li></ol><p>需要传输大文本内容的时候（ POST 请求对数据长度没有要求）</p><h3 id="requests发送post请求语法"><a href="#requests发送post请求语法" class="headerlink" title="requests发送post请求语法"></a>requests发送post请求语法</h3><ul><li>用法：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = requests.post(<span class="string">&#x27;http://www.baidu.com/&#x27;</span>,data = data,headers=headers)</span><br></pre></td></tr></table></figure><ul><li>data 的形式：字典</li></ul><h3 id="POST请求练习"><a href="#POST请求练习" class="headerlink" title="POST请求练习"></a>POST请求练习</h3><blockquote><p>下面面我们通过手机版百度翻译的例子看看post请求如何使用：</p><p>地址：<a href="http://fanyi.baidu.com/">http://fanyi.baidu.com/</a></p></blockquote><p><strong>思路分析</strong></p><ol><li><p>抓包确定请求的url地址</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%911.png"></p></li><li><p>确定请求的参数</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%912.png"></p></li><li><p>确定返回数据的位置</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%913.png"></p></li><li><p>模拟浏览器获取数据</p></li></ol><p><strong>小结</strong></p><p>在模拟登陆等场景，经常需要发送post请求，直接使用<code>requests.post(url,data)</code>即可</p><h3 id="为什么要使用代理"><a href="#为什么要使用代理" class="headerlink" title="为什么要使用代理"></a>为什么要使用代理</h3><ol><li>让服务器以为不是同一个客户端在请求</li><li>防止我们的真实地址被泄露，防止被追究</li></ol><h3 id="理解使用代理的过程"><a href="#理解使用代理的过程" class="headerlink" title="理解使用代理的过程"></a>理解使用代理的过程</h3><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E7%9A%84%E8%BF%87%E7%A8%8B.png"></p><h3 id="理解正向代理和反向代理的区别"><a href="#理解正向代理和反向代理的区别" class="headerlink" title="理解正向代理和反向代理的区别"></a>理解正向代理和反向代理的区别</h3><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86%E5%92%8C%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E7%9A%84%E5%8C%BA%E5%88%AB.png"></p><p>通过上图可以看出：</p><ul><li>正向代理：对于浏览器知道服务器的真实地址，例如VPN</li><li>反向代理：浏览器不知道服务器的真实地址，例如nginx</li></ul><h3 id="代理的使用"><a href="#代理的使用" class="headerlink" title="代理的使用"></a>代理的使用</h3><ul><li><p>用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.get(<span class="string">&quot;http://www.baidu.com&quot;</span>,  proxies = proxies)</span><br></pre></td></tr></table></figure></li><li><p>proxies的形式：字典</p></li><li><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">proxies = &#123; </span><br><span class="line">    <span class="string">&quot;http&quot;</span>: <span class="string">&quot;http://12.34.56.79:9527&quot;</span>, </span><br><span class="line">    <span class="string">&quot;https&quot;</span>: <span class="string">&quot;https://12.34.56.79:9527&quot;</span>, </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="代理IP的分类"><a href="#代理IP的分类" class="headerlink" title="代理IP的分类"></a>代理IP的分类</h3><p>根据代理ip的匿名程度，代理IP可以分为下面四类：</p><ul><li>透明代理(Transparent Proxy)：透明代理虽然可以直接“隐藏”你的IP地址，但是还是可以查到你是谁。</li><li>匿名代理(Anonymous Proxy)：使用匿名代理，别人只能知道你用了代理，无法知道你是谁。</li><li>高匿代理(Elite proxy或High Anonymity Proxy)：高匿代理让别人根本无法发现你是在用代理，所以是最好的选择。</li></ul><p>在使用的使用，毫无疑问使用高匿代理效果最好</p><p>从请求使用的协议可以分为：</p><ul><li>http代理</li><li>https代理</li><li>socket代理等</li></ul><p>不同分类的代理，在使用的时候需要根据抓取网站的协议来选择</p><h3 id="代理IP使用的注意点"><a href="#代理IP使用的注意点" class="headerlink" title="代理IP使用的注意点"></a>代理IP使用的注意点</h3><ul><li><p>反反爬</p><p>使用代理ip是非常必要的一种<code>反反爬</code>的方式</p><p>但是即使使用了代理ip，对方服务器任然会有很多的方式来检测我们是否是一个爬虫，比如：</p><ul><li><p>一段时间内，检测IP访问的频率，访问太多频繁会屏蔽</p></li><li><p>检查Cookie，User-Agent，Referer等header参数，若没有则屏蔽</p></li><li><p>服务方购买所有代理提供商，加入到反爬虫数据库里，若检测是代理则屏蔽</p><p>所以更好的方式在使用代理ip的时候使用随机的方式进行选择使用，不要每次都用一个代理ip</p></li></ul></li><li><p>代理ip池的更新</p><p>购买的代理ip很多时候大部分(超过60%)可能都没办法使用，这个时候就需要通过程序去检测哪些可用，把不能用的删除掉。</p></li></ul><h3 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h3><ol><li>requests发送post请求使用requests.post方法，带上请求体，其中请求体需要时字典的形式，传递给data参数接收</li><li>在requests中使用代理，需要准备字典形式的代理，传递给proxies参数接收</li><li>不同协议的url地址，需要使用不同的代理去请求</li></ol><h2 id="requests模块处理cookie"><a href="#requests模块处理cookie" class="headerlink" title="requests模块处理cookie"></a>requests模块处理cookie</h2><h3 id="爬虫中使用cookie"><a href="#爬虫中使用cookie" class="headerlink" title="爬虫中使用cookie"></a>爬虫中使用cookie</h3><blockquote><p>为了能够通过爬虫获取到登录后的页面，或者是解决通过cookie的反扒，需要使用request来处理cookie相关的请求</p></blockquote><p><strong>爬虫中使用cookie的利弊</strong></p><ol><li>带上cookie的好处<ul><li>能够访问登录后的页面</li><li>能够实现部分反反爬</li></ul></li><li>带上cookie的坏处<ul><li>一套cookie往往对应的是一个用户的信息，请求太频繁有更大的可能性被对方识别为爬虫</li><li>那么上面的问题如何解决 ?使用多个账号</li></ul></li></ol><p><strong>requests处理cookie的方法</strong></p><p>使用requests处理cookie有三种方法：</p><ol><li>cookie字符串放在headers中</li><li>把cookie字典放传给请求方法的cookies参数接收</li><li>使用requests提供的session模块</li></ol><h3 id="cookie添加在heades中"><a href="#cookie添加在heades中" class="headerlink" title="cookie添加在heades中"></a>cookie添加在heades中</h3><p><strong>headers中cookie的位置</strong></p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/cookie.png"></p><ul><li>headers中的cookie：<ul><li>使用分号(;)隔开</li><li>分号两边的类似a&#x3D;b形式的表示一条cookie</li><li>a&#x3D;b中，a表示键（name），b表示值（value）</li><li>在headers中仅仅使用了cookie的name和value</li></ul></li></ul><p><strong>cookie的具体组成的字段</strong></p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/cookie%E5%AD%97%E6%AE%B5.png"></p><p>由于headers中对cookie仅仅使用它的name和value，所以在代码中我们仅仅需要cookie的name和value即可</p><p><strong>在headers中使用cookie</strong></p><p>复制浏览器中的cookie到代码中使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line"><span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&quot;</span>,</span><br><span class="line"><span class="string">&quot;Cookie&quot;</span>:<span class="string">&quot;BIDUPSID=A90C08D3E5A4CF715B19A45EF0C36277; PSTM=1681971547; BAIDUID=A90C08D3E5A4CF710048D190A8AFC834:FG=1; BD_UPN=12314753; ZFY=xt9EwObfUTgPelIsIlSVDjUJToSnzhC5qjcv1rmfSX4:C; BAIDUID_BFESS=A90C08D3E5A4CF710048D190A8AFC834:FG=1; COOKIE_SESSION=7173_1_7_9_8_6_1_0_7_4_0_0_7174_0_6_0_1684733694_1684233838_1684733688%7C9%231930841_3_1684233835%7C2; baikeVisitId=24f4253b-72b8-4f90-9510-1eba650828fe; BA_HECTOR=0581a1a0ag85a12h8h2481e51i6roc81n; BD_HOME=1; H_PS_PSSID=38516_36550_38540_38614_38576_38486_38196_38636_26350&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">requests.get(url,headers=headers)</span><br></pre></td></tr></table></figure><p><strong>注意：</strong></p><p>cookie有过期时间 ，所以直接复制浏览器中的cookie可能意味着下一程序继续运行的时候需要替换代码中的cookie，对应的我们也可以通过一个程序专门来获取cookie供其他程序使用；当然也有很多网站的cookie过期时间很长，这种情况下，直接复制cookie来使用更加简单</p><h3 id="使用cookies参数接收字典形式的cookie"><a href="#使用cookies参数接收字典形式的cookie" class="headerlink" title="使用cookies参数接收字典形式的cookie"></a>使用cookies参数接收字典形式的cookie</h3><ul><li>cookies的形式：字典</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cookies = <span class="punctuation">&#123;</span><span class="attr">&quot;cookie的name&quot;</span><span class="punctuation">:</span><span class="string">&quot;cookie的value&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><ul><li>使用方法：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.get(url,headers=headers,cookies=cookie_dict&#125;</span><br></pre></td></tr></table></figure><h3 id="使用requests-session处理cookie"><a href="#使用requests-session处理cookie" class="headerlink" title="使用requests.session处理cookie"></a>使用requests.session处理cookie</h3><blockquote><p>前面使用手动的方式使用cookie，那么有没有更好的方法在requets中处理cookie呢？</p></blockquote><p>requests 提供了一个叫做session类，来实现客户端和服务端的<code>会话保持</code></p><p>会话保持有两个内涵：</p><ul><li>保存cookie，下一次请求会带上前一次的cookie</li><li>实现和服务端的长连接，加快请求速度</li></ul><p><strong>使用方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session = requests.session()</span><br><span class="line">response = session.get(url,headers)</span><br></pre></td></tr></table></figure><p>session实例在请求了一个网站后，对方服务器设置在本地的cookie会保存在session中，下一次再使用session请求对方服务器的时候，会带上前一次的cookie</p><p><strong>动手练习：</strong></p><p>动手尝试使用session来登录人人网： <a href="http://www.renren.com/PLogin.do">http://www.renren.com/PLogin.do</a> (先不考虑这个url地址从何而来)，请求体的格式：<code>&#123;&quot;email&quot;:&quot;username&quot;, &quot;password&quot;:&quot;password&quot;&#125;</code></p><p><strong>思路分析</strong></p><ol><li>准备url地址和请求参数</li><li>构造session发送post请求</li><li>使用session请求个人主页，观察是否请求成功</li></ol><h3 id="小结-6"><a href="#小结-6" class="headerlink" title="小结"></a>小结</h3><ol><li>cookie字符串可以放在headers字典中，键为Cookie，值为cookie字符串</li><li>可以把cookie字符串转化为字典，使用请求方法的cookies参数接收</li><li>使用requests提供的session模块，能够自动实现cookie的处理，包括请求的时候携带cookie，获取响应的时候保存cookie</li></ol><h2 id="requests的其他方法"><a href="#requests的其他方法" class="headerlink" title="requests的其他方法"></a>requests的其他方法</h2><h3 id="requests中cookirJar的处理方法"><a href="#requests中cookirJar的处理方法" class="headerlink" title="requests中cookirJar的处理方法"></a>requests中cookirJar的处理方法</h3><blockquote><p>使用request获取的resposne对象，具有cookies属性，能够获取对方服务器设置在本地的cookie，但是如何使用这些cookie呢？</p></blockquote><p><strong>方法介绍</strong></p><ol><li>response.cookies是CookieJar类型</li><li>使用requests.utils.dict_from_cookiejar，能够实现把cookiejar对象转化为字典</li></ol><p><strong>方法展示</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line"><span class="comment">#发送请求，获取response</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response.cookies))</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用方法从cookiejar中提取数据</span></span><br><span class="line">cookies = requests.utils.dict_from_cookiejar(response.cookies)</span><br><span class="line"><span class="built_in">print</span>(cookies)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;requests.cookies.RequestsCookieJar&#x27;</span>&gt;</span><br><span class="line">&#123;<span class="string">&#x27;BDORZ&#x27;</span>: <span class="string">&#x27;27315&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong></p><p>在前面的requests的session类中，我们不需要处理cookie的任何细节，如果有需要，我们可以使用上述方法来解决</p><h3 id="requests处理证书错误"><a href="#requests处理证书错误" class="headerlink" title="requests处理证书错误"></a>requests处理证书错误</h3><blockquote><p>经常我们在网上冲浪时，经常能够看到下面的提示：</p></blockquote><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/12306ssl%E9%94%99%E8%AF%AF.png"></p><p>出现这个问题的原因是：ssl的证书不安全导致</p><p><strong>代码中发起请求的效果</strong></p><p>那么如果在代码中请求会怎么样呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.12306.cn/mormhweb/&quot;</span></span><br><span class="line">response = requests.get(url)</span><br></pre></td></tr></table></figure><p>返回证书错误，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssl.CertificateError ...</span><br></pre></td></tr></table></figure><p><strong>解决方案</strong></p><p>为了在代码中能够正常的请求，我们修改添加一个参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.12306.cn/mormhweb/&quot;</span></span><br><span class="line">response = requests.get(url,verify=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="超时参数的使用"><a href="#超时参数的使用" class="headerlink" title="超时参数的使用"></a>超时参数的使用</h3><blockquote><p>在平时网上冲浪的过程中，我们经常会遇到网络波动，这个时候，一个请求等了很久可能任然没有结果</p><p>在爬虫中，一个请求很久没有结果，就会让整个项目的效率变得非常低，这个时候我们就需要对请求进行强制要求，让他必须在特定的时间内返回结果，否则就报错</p></blockquote><p><strong>超时参数使用方法如下：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = requests.get(url,timeout=3)</span><br></pre></td></tr></table></figure><p>通过添加timeout参数，能够保证在3秒钟内返回响应，否则会报错</p><p><strong>注意：</strong></p><p>这个方法还能够拿来检测代理ip的质量，如果一个代理ip在很长时间没有响应，那么添加超时之后也会报错，对应的这个ip就可以从代理ip池中删除</p><h3 id="retrying模块的使用"><a href="#retrying模块的使用" class="headerlink" title="retrying模块的使用"></a>retrying模块的使用</h3><blockquote><p>使用超时参数能够加快我们整体的请求速度，但是在正常的网页浏览过成功，如果发生速度很慢的情况，我们会做的选择是<strong>刷新页面</strong>，那么在代码中，我们是否也可以刷新请求呢？</p></blockquote><p>对应的，retrying模块就可以帮助我们解决</p><p><strong>retrying模块的使用</strong></p><p>retrying模块的地址：<a href="https://pypi.org/project/retrying/">https://pypi.org/project/retrying/</a></p><p>retrying 模块的使用</p><ol><li>使用retrying模块提供的retry模块</li><li>通过装饰器的方式使用，让被装饰的函数反复执行</li><li>retry中可以传入参数<code>stop_max_attempt_number</code>,让函数报错后继续重新执行，达到最大执行次数的上限，如果每次都报错，整个函数报错，如果中间有一个成功，程序继续往后执行</li></ol><p><strong>retrying和requests的简单封装</strong></p><p>实现一个发送请求的函数，每次爬虫中直接调用该函数即可实现发送请求，在其中</p><ul><li>使用timeout实现超时报错</li><li>使用retrying模块实现重试</li></ul><p>代码参考:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> retrying <span class="keyword">import</span> retry</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Parse</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.url_ = <span class="string">&#x27;xxxxxx.com&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#最大重试3次，3次全部报错，才会报错</span></span><br><span class="line"><span class="meta">    @retry(<span class="params">stop_max_attempt_number=<span class="number">3</span></span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_parse_url</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#超时的时候回报错并重试</span></span><br><span class="line">        response = requests.get(self.url_,timeout=<span class="number">3</span>)</span><br><span class="line">        <span class="comment">#状态码不是200，也会报错并重试</span></span><br><span class="line">        <span class="keyword">assert</span> response.status_code == <span class="number">200</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_url</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">try</span>:<span class="comment">#进行异常捕获</span></span><br><span class="line">            response = self._parse_url()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line">            <span class="comment">#报错返回None</span></span><br><span class="line">            response = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parse_ = Parse()</span><br><span class="line">    parse_.parse_url()</span><br></pre></td></tr></table></figure><h3 id="小结-7"><a href="#小结-7" class="headerlink" title="小结"></a>小结</h3><ol><li>requests.utils.dict_from_cookiejar能够实现cookiejar转化为字典</li><li>请求方法中添加verify&#x3D;False能够实现请求过程中不验证证书</li><li>请求方法中添加timeout能够实现强制程序返回结果的能够，否则会报错</li><li>retrying模块能够实现捕获函数的异常，反复执行函数的效果，和timeout配合使用，能够解决网络波动带来的请求不成功的问题</li></ol><h2 id="urllib的学习"><a href="#urllib的学习" class="headerlink" title="urllib的学习"></a>urllib的学习</h2><h3 id="urllib介绍"><a href="#urllib介绍" class="headerlink" title="urllib介绍"></a>urllib介绍</h3><p>除了requests模块可以发送请求之外, urllib模块也可以实现请求的发送,只是操作方法略有不同!</p><p>urllib在python中分为urllib和urllib2，在python3中为urllib</p><p>下面以python3的urllib为例进行讲解</p><h3 id="urllib的基本方法介绍"><a href="#urllib的基本方法介绍" class="headerlink" title="urllib的基本方法介绍"></a>urllib的基本方法介绍</h3><p><strong>urllib.urlopoen</strong></p><ol><li><p>传入URL地址</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = urllib.urlopen(<span class="string">&quot;http://www.baidu.com&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p>传入request对象</p></li></ol><p><strong>urllib.Request</strong></p><ol><li><p>构造简单请求</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构造请求</span></span><br><span class="line">request = urllib.request.Request(<span class="string">&quot;http://www.baidu.com&quot;</span>)</span><br><span class="line"><span class="comment">#发送请求获取响应</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br></pre></td></tr></table></figure></li><li><p>传入headers参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构造headers</span></span><br><span class="line">headers = &#123;<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&quot;</span>&#125;</span><br><span class="line"><span class="comment">#构造请求</span></span><br><span class="line">request = urllib.request.Request(url,headers=headers)</span><br><span class="line"><span class="comment">#发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br></pre></td></tr></table></figure></li><li><p>传入data参数 实现发送post请求</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构造headers</span></span><br><span class="line">headers = &#123;<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&quot;</span>&#125;</span><br><span class="line"><span class="comment">#构造请求体</span></span><br><span class="line">formdata = &#123;</span><br><span class="line"><span class="string">&quot;type&quot;</span>:<span class="string">&quot;AUTO&quot;</span>,</span><br><span class="line"><span class="string">&quot;i&quot;</span>:<span class="string">&quot;i love python&quot;</span>,</span><br><span class="line"><span class="string">&quot;doctype&quot;</span>:<span class="string">&quot;json&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#构造请求</span></span><br><span class="line">request = urllib.request.Request(url,data=formdata,headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="built_in">print</span>(response.read())</span><br></pre></td></tr></table></figure></li></ol><p><strong>response.read()</strong></p><p>获取响应的html字符串,bytes类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#发送请求</span></span><br><span class="line">response = urllib.urlopen(<span class="string">&quot;http://www.baidu.com&quot;</span>)</span><br><span class="line"><span class="comment">#获取响应</span></span><br><span class="line">response.read()</span><br></pre></td></tr></table></figure><p><strong>urllib请求百度首页的完整例子</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line"><span class="comment">#构造headers</span></span><br><span class="line">headers = &#123;<span class="string">&quot;User-Agent&quot;</span> : <span class="string">&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)&quot;</span>&#125;</span><br><span class="line"><span class="comment">#构造请求</span></span><br><span class="line">request = urllib.request.Request(url, headers = headers)</span><br><span class="line"><span class="comment">#发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="comment">#获取html字符串</span></span><br><span class="line">html_str = response.read().decode()</span><br><span class="line"><span class="built_in">print</span>(html_str)</span><br></pre></td></tr></table></figure><h3 id="小结-8"><a href="#小结-8" class="headerlink" title="小结"></a>小结</h3><ol><li>urllib.request中实现了构造请求和发送请求的方法</li><li>urllib.request.Request(url,headers,data)能够构造请求</li><li>urllib.request.urlopen能够接受request请求或者url地址发送请求，获取响应</li><li>response.read()能够实现获取响应中的bytes字符串</li></ol><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/1.2%E6%80%BB%E7%BB%93xmind.png"></p><h1 id="数据提取方法"><a href="#数据提取方法" class="headerlink" title="数据提取方法"></a>数据提取方法</h1><h2 id="数据提取的概念"><a href="#数据提取的概念" class="headerlink" title="数据提取的概念"></a>数据提取的概念</h2><p>Python爬虫通过HTTP请求获取网页内容，然后使用各种技术（如正则表达式、beautifulsoup、pyquery等）从HTML中提取数据。</p><p>数据提取的目的是将网页中的有用信息提取出来。例如，在爬取电商网站时，可以提取商品名称、价格、描述、用户评价等信息。在社交媒体上爬取时，可以提取用户信息、关注数、转发数等信息。通过提取这些有用的数据，可以进行数据分析、数据挖掘、信息展示等操作。</p><h3 id="爬虫中数据的分类"><a href="#爬虫中数据的分类" class="headerlink" title="爬虫中数据的分类"></a>爬虫中数据的分类</h3><blockquote><p>在爬虫爬取的数据中有很多不同类型的数据,我们需要了解数据的不同类型来又规律的提取和解析数据.</p></blockquote><ul><li>结构化数据：json，xml等<ul><li>处理方式：直接转化为python类型</li></ul></li><li>非结构化数据：HTML<ul><li>处理方式：正则表达式、xpath</li></ul></li></ul><p>下面以今日头条的首页为例，介绍结构化数据和非结构化数据</p><ul><li><p>结构化数据例子：</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE.png"></p></li><li><p>非结构化数据：</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE.png"></p></li><li><p>XML数据：</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;bookstore&gt;</span><br><span class="line">&lt;book category=&quot;COOKING&quot;&gt;</span><br><span class="line">  &lt;title lang=&quot;en&quot;&gt;Everyday Italian&lt;/title&gt; </span><br><span class="line">  &lt;author&gt;Giada De Laurentiis&lt;/author&gt; </span><br><span class="line">  &lt;year&gt;2005&lt;/year&gt; </span><br><span class="line">  &lt;price&gt;30.00&lt;/price&gt; </span><br><span class="line">&lt;/book&gt;</span><br><span class="line">&lt;book category=&quot;CHILDREN&quot;&gt;</span><br><span class="line">  &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt; </span><br><span class="line">  &lt;author&gt;J K. Rowling&lt;/author&gt; </span><br><span class="line">  &lt;year&gt;2005&lt;/year&gt; </span><br><span class="line">  &lt;price&gt;29.99&lt;/price&gt; </span><br><span class="line">&lt;/book&gt;</span><br><span class="line">&lt;book category=&quot;WEB&quot;&gt;</span><br><span class="line">  &lt;title lang=&quot;en&quot;&gt;Learning XML&lt;/title&gt; </span><br><span class="line">  &lt;author&gt;Erik T. Ray&lt;/author&gt; </span><br><span class="line">  &lt;year&gt;2003&lt;/year&gt; </span><br><span class="line">  &lt;price&gt;39.95&lt;/price&gt; </span><br><span class="line">&lt;/book&gt;</span><br><span class="line">&lt;/bookstore&gt;</span><br></pre></td></tr></table></figure><p>从上面可以看出，xml数据也是结构非常明显的</p><h3 id="小结-9"><a href="#小结-9" class="headerlink" title="小结"></a>小结</h3><ol><li>爬虫中数据分类之结构化数据: json,xml</li><li>爬虫中数据分类之非结构化数据:Html,字符串</li><li>结构化数据处理的方式有:jsonpath,xpath,转换python类型处理,bs4</li><li>非结构化数据处理方式有:正则表达式,xpath,bs4</li></ol><h2 id="json的数据提取"><a href="#json的数据提取" class="headerlink" title="json的数据提取"></a>json的数据提取</h2><p>JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。</p><h3 id="json模块中方法的学习"><a href="#json模块中方法的学习" class="headerlink" title="json模块中方法的学习"></a>json模块中方法的学习</h3><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/json%E7%9A%84%E6%96%B9%E6%B3%95.png"></p><p>其中类文件对象的理解：</p><blockquote><p>具有read()或者write()方法的对象就是类文件对象，比如f&#x3D;open(‘a.txt’,’r’)f就是类文件对象</p></blockquote><p>具体使用发法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># json.dumps实现python类型转化为json字符串</span></span><br><span class="line"><span class="comment"># indent实现换行和空格</span></span><br><span class="line"><span class="comment"># ensure_ascii=False实现让中文写入的时候保持为中文</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#json.loads 实现json字符串转化为python的数据类型</span></span><br><span class="line">my_dict = json.loads(json_str)</span><br><span class="line"></span><br><span class="line"><span class="comment">#json.dump 实现把python类型写入类文件对象</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;temp.txt&quot;</span>,<span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(mydict,f,ensure_ascii=<span class="literal">False</span>,indent=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># json.load 实现类文件对象中的json字符串转化为python类型</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;temp.txt&quot;</span>,<span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    my_dict = json.load(f)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注意:</span><br><span class="line">json.dumps()和json.dump()，json.loads()和json.load()的区别有s的是直接和json数据打交道；没有s的是和json文件打交道</span><br><span class="line"></span><br><span class="line">python数据 &gt;&gt; json数据 : json.dumps(python数据,ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">python数据 &gt;&gt; json文件 ：json.dump(python字典,json文件对象,ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">json数据  &gt;&gt; python数据：json.loads(json数据)</span><br><span class="line">json文件  &gt;&gt; python数据：json.load(json文件对象)</span><br></pre></td></tr></table></figure><h3 id="jsonpath模块的学习"><a href="#jsonpath模块的学习" class="headerlink" title="jsonpath模块的学习"></a>jsonpath模块的学习</h3><p>用来解析多层嵌套的json数据;JsonPath 是一种信息抽取类库，是从JSON文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript, Python， PHP 和 Java。</p><p><strong>JsonPath 对于 JSON 来说，相当于 XPath 对于 XML。</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">安装方法：pip install jsonpath</span><br><span class="line"></span><br><span class="line">官方文档：http://goessner.net/articles/JsonPath</span><br></pre></td></tr></table></figure><p><strong>JsonPath与XPath语法对比：</strong></p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/jsonpath%E7%9A%84%E6%96%B9%E6%B3%95.png"></p><p><strong>语法使用示例</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&#123; &quot;store&quot;: &#123;</span><br><span class="line">    &quot;book&quot;: [ </span><br><span class="line">      &#123; &quot;category&quot;: &quot;reference&quot;,</span><br><span class="line">        &quot;author&quot;: &quot;Nigel Rees&quot;,</span><br><span class="line">        &quot;title&quot;: &quot;Sayings of the Century&quot;,</span><br><span class="line">        &quot;price&quot;: 8.95</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123; &quot;category&quot;: &quot;fiction&quot;,</span><br><span class="line">        &quot;author&quot;: &quot;Evelyn Waugh&quot;,</span><br><span class="line">        &quot;title&quot;: &quot;Sword of Honour&quot;,</span><br><span class="line">        &quot;price&quot;: 12.99</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123; &quot;category&quot;: &quot;fiction&quot;,</span><br><span class="line">        &quot;author&quot;: &quot;Herman Melville&quot;,</span><br><span class="line">        &quot;title&quot;: &quot;Moby Dick&quot;,</span><br><span class="line">        &quot;isbn&quot;: &quot;0-553-21311-3&quot;,</span><br><span class="line">        &quot;price&quot;: 8.99</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123; &quot;category&quot;: &quot;fiction&quot;,</span><br><span class="line">        &quot;author&quot;: &quot;J. R. R. Tolkien&quot;,</span><br><span class="line">        &quot;title&quot;: &quot;The Lord of the Rings&quot;,</span><br><span class="line">        &quot;isbn&quot;: &quot;0-395-19395-8&quot;,</span><br><span class="line">        &quot;price&quot;: 22.99</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;bicycle&quot;: &#123;</span><br><span class="line">      &quot;color&quot;: &quot;red&quot;,</span><br><span class="line">      &quot;price&quot;: 19.95</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/jsonpath%E4%BE%8B%E5%AD%90%E8%AF%B4%E6%98%8E.png"></p><p><strong>代码示例：</strong></p><blockquote><p>我们以拉勾网城市JSON文件 <a href="http://www.lagou.com/lbs/getAllCitySearchLabels.json">http://www.lagou.com/lbs/getAllCitySearchLabels.json</a> 为例，获取所有城市。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> jsonpath</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.lagou.com/lbs/getAllCitySearchLabels.json&#x27;</span></span><br><span class="line">response =requests.get(url)</span><br><span class="line">html_str = response.content.decode()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把json格式字符串转换成python对象</span></span><br><span class="line">jsonobj = json.loads(html_str)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从根节点开始，匹配name节点</span></span><br><span class="line">citylist = jsonpath.jsonpath(jsonobj,<span class="string">&#x27;$..name&#x27;</span>)</span><br><span class="line"></span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;city.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"></span><br><span class="line">content = json.dumps(citylist, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">fp.write(content.encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">fp.close()</span><br></pre></td></tr></table></figure><h3 id="小结-10"><a href="#小结-10" class="headerlink" title="小结"></a>小结</h3><ol><li>json的概念(JavaScript Object Notation)和json的作用数据交互时的一种数据格式</li><li>json模块中操作字符串和python类型互转的方法是dump,load</li><li>json模块中操作文件和python类型互转的方法是dumps,loads</li><li>jsonpath模块的安装 pip install jsonpath</li><li>jsonpath的解析根节点:$</li><li>jsonpath的解析子节点:.</li></ol><h2 id="数据提取之正则"><a href="#数据提取之正则" class="headerlink" title="数据提取之正则"></a>数据提取之正则</h2><h3 id="什么是正则表达式"><a href="#什么是正则表达式" class="headerlink" title="什么是正则表达式"></a>什么是正则表达式</h3><p>用事先定义好的一些特定字符、及这些特定字符的组合，组成一个<strong>规则字符串</strong>，这个<strong>规则字符串</strong>用来表达对字符串的一种<strong>过滤</strong>逻辑。</p><h3 id="正则表达式的常见语法"><a href="#正则表达式的常见语法" class="headerlink" title="正则表达式的常见语法"></a>正则表达式的常见语法</h3><p>知识点</p><ul><li>正则中的字符</li><li>正则中的预定义字符集</li><li>正则中的数量词</li></ul><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E8%AF%AD%E6%B3%95.png"></p><p>正则的语法很多，不能够全部复习，对于其他的语法，可以临时查阅资料，比如:表示或还能使用<code>|</code></p><p>练习： 下面的输出是什么？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">string_a = <span class="string">&#x27;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot;&gt;\n\t\t&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;&gt;\n\t\t&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;&gt;\n        &lt;meta name=&quot;theme-color&quot; content=&quot;#2932e1&quot;&gt;&#x27;</span></span><br><span class="line">ret = re.findall(<span class="string">&quot;&lt;.*&gt;&quot;</span>,string_a)</span><br><span class="line"><span class="built_in">print</span>(ret)</span><br></pre></td></tr></table></figure><h3 id="re模块的常见方法"><a href="#re模块的常见方法" class="headerlink" title="re模块的常见方法"></a>re模块的常见方法</h3><ul><li><p>pattern.match（从头找一个）</p></li><li><p>pattern.search（找一个）</p></li><li><p>pattern.findall（找所有）</p><ul><li>返回一个列表，没有就是空列表</li><li><code>re.findall(&quot;\d&quot;,&quot;chuan1zhi2&quot;) &gt;&gt; [&quot;1&quot;,&quot;2&quot;]</code></li></ul></li><li><p>pattern.sub（替换）</p><ul><li><code>re.sub(&quot;\d&quot;,&quot;_&quot;,&quot;chuan1zhi2&quot;) &gt;&gt; [&quot;chuan_zhi_&quot;]</code></li></ul></li><li><p>re.compile（编译）</p><ul><li><p>返回一个模型P，具有和re一样的方法，但是传递的参数不同</p></li><li><p>匹配模式需要传到compile中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = re.<span class="built_in">compile</span>(<span class="string">&quot;\d&quot;</span>,re.S)</span><br><span class="line">p.findall(<span class="string">&quot;chuan1zhi2&quot;</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="python中原始字符串r的用法"><a href="#python中原始字符串r的用法" class="headerlink" title="python中原始字符串r的用法"></a>python中原始字符串r的用法</h3><p>原始字符串定义(raw string)：所有的字符串都是直接按照字面的意思来使用，没有转义特殊或不能打印的字符，原始字符串往往针对特殊字符而言。例如<code>&quot;\n&quot;</code>的原始字符串就是<code>&quot;\\n&quot;</code></p><ul><li>原始字符串的长度</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">19</span>]: <span class="built_in">len</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">Out[<span class="number">19</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: <span class="built_in">len</span>(<span class="string">r&quot;\n&quot;</span>)</span><br><span class="line">Out[<span class="number">20</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: <span class="string">r&quot;\n&quot;</span>[<span class="number">0</span>]</span><br><span class="line">Out[<span class="number">21</span>]: <span class="string">&#x27;\\&#x27;</span></span><br></pre></td></tr></table></figure><ul><li>正则中原始字符串的使用</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: <span class="string">r&quot;a\nb&quot;</span> == <span class="string">&quot;a\\nb&quot;</span></span><br><span class="line">Out[<span class="number">13</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: re.findall(<span class="string">&quot;a\nb&quot;</span>,<span class="string">&quot;a\nb&quot;</span>)</span><br><span class="line">Out[<span class="number">14</span>]: [<span class="string">&#x27;a\nb&#x27;</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: re.findall(<span class="string">r&quot;a\nb&quot;</span>,<span class="string">&quot;a\nb&quot;</span>)</span><br><span class="line">Out[<span class="number">15</span>]: [<span class="string">&#x27;a\nb&#x27;</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: re.findall(<span class="string">&quot;a\\nb&quot;</span>,<span class="string">&quot;a\nb&quot;</span>)</span><br><span class="line">Out[<span class="number">16</span>]: [<span class="string">&#x27;a\nb&#x27;</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: re.findall(<span class="string">&quot;a\\nb&quot;</span>,<span class="string">&quot;a\\nb&quot;</span>)</span><br><span class="line">Out[<span class="number">17</span>]: []</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: re.findall(<span class="string">r&quot;a\\nb&quot;</span>,<span class="string">&quot;a\\nb&quot;</span>)</span><br><span class="line">Out[<span class="number">18</span>]: [<span class="string">&#x27;a\\nb&#x27;</span>]</span><br></pre></td></tr></table></figure><p><strong>上面的现象说明什么？</strong></p><ul><li><p>正则中使用原始字符串<code>r</code>能够忽略转义符号带来的影响，加上原始字符串<code>r</code>之后，待匹配的字符串中有多少个<code>\</code>，正则中就添加多少个<code>\</code>即可</p></li><li><p>windows中原始字符串r的使用</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/windows%E5%8E%9F%E5%A7%8B%E5%AD%97%E7%AC%A6%E4%B8%B2r.png"></p></li></ul><h3 id="匹配中文"><a href="#匹配中文" class="headerlink" title="匹配中文"></a>匹配中文</h3><blockquote><p>在某些情况下，我们想匹配文本中的汉字，有一点需要注意的是，中文的 unicode 编码范围 主要在 [u4e00-u9fa5]，这里说主要是因为这个范围并不完整，比如没有包括全角（中文）标点，不过，在大部分情况下，应该是够用的。</p></blockquote><p>假设现在想把字符串 title &#x3D; u’你好，hello，世界’ 中的中文提取出来，可以这么做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">title = <span class="string">u&#x27;你好，hello，世界&#x27;</span></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(u<span class="string">r&#x27;[\u4e00-\u9fa5]+&#x27;</span>)</span><br><span class="line">result = pattern.findall(title)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意点: 中文匹配 需要设置unicode字符才可以匹配</span></span><br></pre></td></tr></table></figure><h3 id="小结-11"><a href="#小结-11" class="headerlink" title="小结"></a>小结</h3><ol><li>熟悉正则表达式中字符:\d,\D,\w,\W,,.等</li><li>熟悉正则表达式中量词:*,+,?,{}等</li><li>熟悉正则表达式中范围:[‘值1’,’值2’]</li><li>re模块的常见方法:match,search,find,findall)</li><li>原始字符串<code>r</code>的用法(保持原先字符串中所有的字符)</li><li>匹配中文的操作[u4e00-u9fa5],中文比配需要注意:汉字和正则表达式都需要是unicode字符操作</li></ol><h2 id="数据提取之xpath"><a href="#数据提取之xpath" class="headerlink" title="数据提取之xpath"></a>数据提取之xpath</h2><p>lxml是一款高性能的 Python HTML&#x2F;XML 解析器，我们可以利用XPath，来快速的定位特定元素以及获取节点信息</p><h3 id="什么是xpath"><a href="#什么是xpath" class="headerlink" title="什么是xpath"></a>什么是xpath</h3><p>XPath (XML Path Language) 是一门在 HTML\XML 文档中查找信息的<strong>语言</strong>，可用来在 HTML\XML 文档中对<strong>元素和属性进行遍历</strong>。</p><p>W3School官方文档：<a href="http://www.w3school.com.cn/xpath/index.asp">http://www.w3school.com.cn/xpath/index.asp</a></p><h3 id="认识xml"><a href="#认识xml" class="headerlink" title="认识xml"></a>认识xml</h3><p>知识点：</p><ul><li>html和xml的区别</li><li>xml中各个元素的的关系和属性</li></ul><p><strong>html和xml的区别</strong></p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E8%AE%A4%E8%AF%86xml.png"></p><h3 id="xml的树结构"><a href="#xml的树结构" class="headerlink" title="xml的树结构"></a>xml的树结构</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bookstore</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">book</span> <span class="attr">category</span>=<span class="string">&quot;COOKING&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span>Everyday Italian<span class="tag">&lt;/<span class="name">title</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">author</span>&gt;</span>Giada De Laurentiis<span class="tag">&lt;/<span class="name">author</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">year</span>&gt;</span>2005<span class="tag">&lt;/<span class="name">year</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">price</span>&gt;</span>30.00<span class="tag">&lt;/<span class="name">price</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">book</span> <span class="attr">category</span>=<span class="string">&quot;CHILDREN&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span>Harry Potter<span class="tag">&lt;/<span class="name">title</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">author</span>&gt;</span>J K. Rowling<span class="tag">&lt;/<span class="name">author</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">year</span>&gt;</span>2005<span class="tag">&lt;/<span class="name">year</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">price</span>&gt;</span>29.99<span class="tag">&lt;/<span class="name">price</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">book</span> <span class="attr">category</span>=<span class="string">&quot;WEB&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span>Learning XML<span class="tag">&lt;/<span class="name">title</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">author</span>&gt;</span>Erik T. Ray<span class="tag">&lt;/<span class="name">author</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">year</span>&gt;</span>2003<span class="tag">&lt;/<span class="name">year</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">price</span>&gt;</span>39.95<span class="tag">&lt;/<span class="name">price</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bookstore</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上面的xml内容可以表示为下面的树结构</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E6%A0%91%E7%BB%93%E6%9E%84.png"></p><p>上面的这种结构关系在xpath被进一步细化</p><h3 id="xpath的节点关系"><a href="#xpath的节点关系" class="headerlink" title="xpath的节点关系"></a>xpath的节点关系</h3><p>知识点：</p><ul><li>认识xpath中的节点</li><li>了解xpath中节点之间的关系</li></ul><p><strong>xpath中的节点是什么</strong></p><p>每个XML的标签我们都称之为节点，其中最顶层的节点称为根节点。</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E8%8A%82%E7%82%B9.png"></p><p><strong>xpath中节点的关系</strong></p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/xpath%E4%B8%AD%E8%8A%82%E7%82%B9%E7%9A%84%E5%85%B3%E7%B3%BB.png"></p><h3 id="xpath中节点选择的工具"><a href="#xpath中节点选择的工具" class="headerlink" title="xpath中节点选择的工具"></a>xpath中节点选择的工具</h3><ul><li>Chrome插件 XPath Helper<ul><li>下载地址：<a href="https://pan.baidu.com/s/1UM94dcwgus4SgECuoJ-Jcg">https://pan.baidu.com/s/1UM94dcwgus4SgECuoJ-Jcg</a> 密码:337b</li></ul></li><li>Firefox插件 XPath Checker</li></ul><p>注意： 这些工具是用来<strong>学习xpath语法</strong>的，他们都是<strong>从elements中</strong>匹配数据，elements中的数据和url地址对应的响应不相同，所以在代码中，不建议使用这些工具进行数据的提取</p><h3 id="xpath语法"><a href="#xpath语法" class="headerlink" title="xpath语法"></a>xpath语法</h3><p>知识点</p><ul><li>掌握元素路径的相关方法</li><li>掌握获取获取属性的方法</li><li>掌握获取文本的方法</li></ul><p>我们将在下面的例子中使用这个 XML 文档。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bookstore</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">book</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">&quot;eng&quot;</span>&gt;</span>Harry Potter<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">price</span>&gt;</span>29.99<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">book</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">&quot;eng&quot;</span>&gt;</span>Learning XML<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">price</span>&gt;</span>39.95<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">bookstore</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>选取节点</strong></p><p>XPath 使用路径表达式来选取 XML 文档中的节点或者节点集。这些路径表达式和我们在常规的<strong>电脑文件系统中看到的表达式</strong>非常相似。</p><p><strong>使用chrome插件选择标签时候，选中时，选中的标签会添加属性class&#x3D;”xh-highlight”</strong></p><p><strong>下面列出了最有用的表达式：</strong></p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/xpath%E5%B8%B8%E7%94%A8%E8%A1%A8%E8%BE%BE%E5%BC%8F.png"></p><p><strong>实例</strong></p><p>在下面的表格中，我们已列出了一些路径表达式以及表达式的结果：</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%BB%93%E6%9E%9C.png"></p><p><strong>查找特定的节点</strong></p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/xpath%E6%9F%A5%E6%89%BE%E7%89%B9%E5%AE%9A%E8%8A%82%E7%82%B9.png"></p><p>注意点: 在xpath中，第一个元素的位置是1，最后一个元素的位置是last(),倒数第二个是last()-1</p><p><strong>选取未知节点</strong></p><p>XPath 通配符可用来选取未知的 XML 元素。</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/xpath%E9%80%89%E5%8F%96%E6%9C%AA%E7%9F%A5%E8%8A%82%E7%82%B9.png"></p><p><strong>选取若干路径</strong></p><p>通过在路径表达式中使用“|”运算符，您可以选取若干个路径。</p><p><strong>实例</strong></p><p>在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/xpath%E9%80%89%E5%8F%96%E8%8B%A5%E5%B9%B2%E8%B7%AF%E5%8A%B2.png"></p><h3 id="小结-12"><a href="#小结-12" class="headerlink" title="小结"></a>小结</h3><ol><li>xpath的概述XPath (XML Path Language),解析查找提取信息的语言</li><li>xml是和服务器交互的数据格式和json的作用一致</li><li>html是浏览器解析标签数据显示给用户</li><li>xpath的节点关系:根节点,子节点,父节点,兄弟节点,子节点,后代节点</li><li>xpath的重点语法获取任意节点:<code>//</code></li><li>xpath的重点语法根据属性获取节点:<code>标签[@属性 = &#39;值&#39;]</code></li><li>xpath的获取节点属性值:<code>@属性值</code></li><li>xpath的获取节点文本值:<code>text()</code></li></ol><h2 id="lxml模块的学习"><a href="#lxml模块的学习" class="headerlink" title="lxml模块的学习"></a>lxml模块的学习</h2><h3 id="lxml的安装"><a href="#lxml的安装" class="headerlink" title="lxml的安装"></a>lxml的安装</h3><p>安装方式：<code>pip install lxml</code></p><h3 id="lxml模块的入门使用"><a href="#lxml模块的入门使用" class="headerlink" title="lxml模块的入门使用"></a>lxml模块的入门使用</h3><ol><li><p>导入lxml 的 etree 库 (导入没有提示不代表不能用)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`from lxml import etree`</span><br></pre></td></tr></table></figure></li><li><p>利用etree.HTML，将字符串转化为Element对象,Element对象具有xpath的方法,返回结果的列表，能够接受bytes类型的数据和str类型的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">html = etree.HTML(text) </span><br><span class="line">ret_list = html.xpath(<span class="string">&quot;xpath字符串&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p>把转化后的element对象转化为字符串，返回bytes类型结果 <code>etree.tostring(element)</code></p></li></ol><p>假设我们现有如下的html字符换，尝试对他进行操作</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span> <span class="tag">&lt;<span class="name">ul</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link1.html&quot;</span>&gt;</span>first item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link2.html&quot;</span>&gt;</span>second item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-inactive&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link3.html&quot;</span>&gt;</span>third item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link4.html&quot;</span>&gt;</span>fourth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link5.html&quot;</span>&gt;</span>fifth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span> # 注意，此处缺少一个 <span class="tag">&lt;/<span class="name">li</span>&gt;</span> 闭合标签 </span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span> <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">from lxml import etree</span><br><span class="line">text = &#x27;&#x27;&#x27; <span class="tag">&lt;<span class="name">div</span>&gt;</span> <span class="tag">&lt;<span class="name">ul</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link1.html&quot;</span>&gt;</span>first item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link2.html&quot;</span>&gt;</span>second item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-inactive&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link3.html&quot;</span>&gt;</span>third item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link4.html&quot;</span>&gt;</span>fourth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link5.html&quot;</span>&gt;</span>fifth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;/<span class="name">ul</span>&gt;</span> <span class="tag">&lt;/<span class="name">div</span>&gt;</span> &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">print(type(html)) </span><br><span class="line"></span><br><span class="line">handeled_html_str = etree.tostring(html).decode()</span><br><span class="line">print(handeled_html_str)</span><br></pre></td></tr></table></figure><p>输出为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &#x27;lxml.etree._Element&#x27;&gt;</span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;div&gt; &lt;ul&gt; </span><br><span class="line">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; </span><br><span class="line">        &lt;/li&gt;&lt;/ul&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>可以发现，lxml确实能够把确实的标签补充完成，但是请注意<strong>lxml是人写的，很多时候由于网页不够规范，或者是lxml的bug，即使参考url地址对应的响应去提取数据，任然获取不到，这个时候我们需要使用etree.tostring的方法，观察etree到底把html转化成了什么样子，即根据转化后的html字符串去进行数据的提取</strong>。</p><h3 id="lxml的深入练习"><a href="#lxml的深入练习" class="headerlink" title="lxml的深入练习"></a>lxml的深入练习</h3><blockquote><p>接下来我们继续操作，假设每个class为item-1的li标签是1条新闻数据，如何把这条新闻数据组成一个字典</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">&#x27;&#x27;&#x27; &lt;div&gt; &lt;ul&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; &lt;/div&gt; &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取href的列表和title的列表</span></span><br><span class="line">href_list = html.xpath(<span class="string">&quot;//li[@class=&#x27;item-1&#x27;]/a/@href&quot;</span>)</span><br><span class="line">title_list = html.xpath(<span class="string">&quot;//li[@class=&#x27;item-1&#x27;]/a/text()&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#组装成字典</span></span><br><span class="line"><span class="keyword">for</span> href <span class="keyword">in</span> href_list:</span><br><span class="line">    item = &#123;&#125;</span><br><span class="line">    item[<span class="string">&quot;href&quot;</span>] = href</span><br><span class="line">    item[<span class="string">&quot;title&quot;</span>] = title_list[href_list.index(href)]</span><br><span class="line">    <span class="built_in">print</span>(item)</span><br></pre></td></tr></table></figure><p>输出为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;href&#x27;</span>: <span class="string">&#x27;link1.html&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;first item&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;href&#x27;</span>: <span class="string">&#x27;link2.html&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;second item&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;href&#x27;</span>: <span class="string">&#x27;link4.html&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;fourth item&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>假设在某种情况下，某个新闻的href没有，那么会怎样呢？</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">&#x27;&#x27;&#x27; &lt;div&gt; &lt;ul&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a&gt;first item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; &lt;/div&gt; &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>结果是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;href&#x27;: &#x27;link2.html&#x27;, &#x27;title&#x27;: &#x27;first item&#x27;&#125;</span><br><span class="line">&#123;&#x27;href&#x27;: &#x27;link4.html&#x27;, &#x27;title&#x27;: &#x27;second item&#x27;&#125;</span><br></pre></td></tr></table></figure><p>数据的对应全部错了，这不是我们想要的，接下来通过2.3小节的学习来解决这个问题</p><h3 id="lxml模块的进阶使用"><a href="#lxml模块的进阶使用" class="headerlink" title="lxml模块的进阶使用"></a>lxml模块的进阶使用</h3><blockquote><p>前面我们取到属性，或者是文本的时候，返回字符串 但是如果我们取到的是<strong>一个节点</strong>，返回什么呢?</p></blockquote><p><strong>返回的是element对象，可以继续使用xpath方法</strong>，对此我们可以在后面的数据提取过程中：<strong>先根据某个标签进行分组，分组之后再进行数据的提取</strong></p><p>示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">&#x27;&#x27;&#x27; &lt;div&gt; &lt;ul&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a&gt;first item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; &lt;/div&gt; &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line"></span><br><span class="line">li_list = html.xpath(<span class="string">&quot;//li[@class=&#x27;item-1&#x27;]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(li_list)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&lt;Element li at 0x11106cb48&gt;, &lt;Element li at 0x11106cb88&gt;, &lt;Element li at 0x11106cbc8&gt;]</span><br></pre></td></tr></table></figure><p>可以发现结果是一个element对象，这个对象能够继续使用xpath方法</p><p>先根据li标签进行分组，之后再进行数据的提取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">text = <span class="string">&#x27;&#x27;&#x27; &lt;div&gt; &lt;ul&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a&gt;first item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; &lt;/div&gt; &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#根据li标签进行分组</span></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line">li_list = html.xpath(<span class="string">&quot;//li[@class=&#x27;item-1&#x27;]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在每一组中继续进行数据的提取</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">    item = &#123;&#125;</span><br><span class="line">    item[<span class="string">&quot;href&quot;</span>] = li.xpath(<span class="string">&quot;./a/@href&quot;</span>)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(li.xpath(<span class="string">&quot;./a/@href&quot;</span>))&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    item[<span class="string">&quot;title&quot;</span>] = li.xpath(<span class="string">&quot;./a/text()&quot;</span>)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(li.xpath(<span class="string">&quot;./a/text()&quot;</span>))&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="built_in">print</span>(item)</span><br></pre></td></tr></table></figure><p>结果是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;href&#x27;: None, &#x27;title&#x27;: &#x27;first item&#x27;&#125;</span><br><span class="line">&#123;&#x27;href&#x27;: &#x27;link2.html&#x27;, &#x27;title&#x27;: &#x27;second item&#x27;&#125;</span><br><span class="line">&#123;&#x27;href&#x27;: &#x27;link4.html&#x27;, &#x27;title&#x27;: &#x27;fourth item&#x27;&#125;</span><br></pre></td></tr></table></figure><p>前面的代码中，进行数据提取需要判断，可能某些一面不存在数据的情况，对应的可以使用三元运算符来解决</p><p>以上提取数据的方式：先分组再提取，都会是我们进行数据的提取的主要方法</p><h3 id="小结-13"><a href="#小结-13" class="headerlink" title="小结"></a>小结</h3><ol><li>lxml库的安装: <code>pip install lxml</code></li><li>lxml的导包:<code>from lxml import etree</code>;</li><li>lxml转换解析类型的方法:<code>etree.HTML(text)</code></li><li>lxml解析数据的方法:<code>data.xpath(&quot;//div/text()&quot;)</code></li><li>需要注意lxml提取完毕数据的数据类型都是列表类型</li><li>如果数据比较复杂:先提取大节点, 在遍历小节点操作</li></ol><h2 id="BeautifulSoup4的学习"><a href="#BeautifulSoup4的学习" class="headerlink" title="BeautifulSoup4的学习"></a>BeautifulSoup4的学习</h2><blockquote><p>由于xpath解析数据需要对html结构有深刻的理解,可能对部分同学产生了学习压力, 那么是不是还有其他的解析方法呢?接下来我们学习使用css选择器解析数据的操作库是 BeautifulSoup4</p></blockquote><h3 id="CSS-选择器：BeautifulSoup4的介绍和安装"><a href="#CSS-选择器：BeautifulSoup4的介绍和安装" class="headerlink" title="CSS 选择器：BeautifulSoup4的介绍和安装"></a>CSS 选择器：BeautifulSoup4的介绍和安装</h3><p>和 lxml 一样，Beautiful Soup 也是一个HTML&#x2F;XML的解析器，主要的功能也是如何解析和提取 HTML&#x2F;XML 数据。</p><p>lxml 只会局部遍历，而Beautiful Soup 是基于HTML DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml。</p><p>BeautifulSoup 用来解析 HTML 比较简单，API非常人性化，支持CSS选择器、Python标准库中的HTML解析器，也支持 lxml 的 XML解析器。</p><p>Beautiful Soup 3 目前已经停止开发，推荐现在的项目使用Beautiful Soup 4。使用 pip 安装即可： <code>pip install beautifulsoup4</code></p><p>官方文档：<a href="http://beautifulsoup.readthedocs.io/zh_CN/v4.4.0">http://beautifulsoup.readthedocs.io/zh_CN/v4.4.0</a></p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7.png"></p><h3 id="bs4的基本使用示例"><a href="#bs4的基本使用示例" class="headerlink" title="bs4的基本使用示例"></a>bs4的基本使用示例</h3><p>首先必须要导入 bs4 库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = &quot;&quot;&quot;</span><br><span class="line">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;</span><br><span class="line">&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were</span><br><span class="line">&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,</span><br><span class="line">&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and</span><br><span class="line">&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;</span><br><span class="line">and they lived at the bottom of a well.&lt;/p&gt;</span><br><span class="line">&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">#创建 Beautiful Soup 对象</span><br><span class="line">soup = BeautifulSoup(html)</span><br><span class="line"></span><br><span class="line">#打开本地 HTML 文件的方式来创建对象</span><br><span class="line">#soup = BeautifulSoup(open(&#x27;index.html&#x27;))</span><br><span class="line"></span><br><span class="line">#格式化输出 soup 对象的内容</span><br><span class="line">print soup.prettify()</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line"> &lt;head&gt;</span><br><span class="line">  &lt;title&gt;</span><br><span class="line">   The Dormouse&#x27;s story</span><br><span class="line">  &lt;/title&gt;</span><br><span class="line"> &lt;/head&gt;</span><br><span class="line"> &lt;body&gt;</span><br><span class="line">  &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;</span><br><span class="line">   &lt;b&gt;</span><br><span class="line">    The Dormouse&#x27;s story</span><br><span class="line">   &lt;/b&gt;</span><br><span class="line">  &lt;/p&gt;</span><br><span class="line">  &lt;p class=&quot;story&quot;&gt;</span><br><span class="line">   Once upon a time there were three little sisters; and their names were</span><br><span class="line">   &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;</span><br><span class="line">    &lt;!-- Elsie --&gt;</span><br><span class="line">   &lt;/a&gt;</span><br><span class="line">   ,</span><br><span class="line">   &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;</span><br><span class="line">    Lacie</span><br><span class="line">   &lt;/a&gt;</span><br><span class="line">   and</span><br><span class="line">   &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;</span><br><span class="line">    Tillie</span><br><span class="line">   &lt;/a&gt;</span><br><span class="line">   ;</span><br><span class="line">and they lived at the bottom of a well.</span><br><span class="line">  &lt;/p&gt;</span><br><span class="line">  &lt;p class=&quot;story&quot;&gt;</span><br><span class="line">   ...</span><br><span class="line">  &lt;/p&gt;</span><br><span class="line"> &lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong></p><p>如果我们在 Python 下执行，会看到这样一段警告： <img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/bs4%E8%AD%A6%E5%91%8A.png" alt="警告"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">意思是，如果我们没有显式地指定解析器，所以默认使用这个系统的最佳可用HTML解析器(“lxml”)。如果你在另一个系统中运行这段代码，或者在不同的虚拟环境中，使用不同的解析器造成行为不同。</span><br><span class="line"></span><br><span class="line">但是我们可以通过soup = BeautifulSoup(html,“lxml”)方式指定lxml解析器。</span><br></pre></td></tr></table></figure><h3 id="搜索文档树"><a href="#搜索文档树" class="headerlink" title="搜索文档树"></a>搜索文档树</h3><p>**find_all(name, attrs, recursive, text, <strong>kwargs)</strong></p><p><strong>1) name 参数</strong></p><p>name 参数可以查找所有名字为 name 的tag</p><p><strong>A 传字符串</strong></p><p>最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的<strong>标签:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all(&#x27;b&#x27;)</span><br><span class="line"># [&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;]</span><br><span class="line"></span><br><span class="line">print soup.find_all(&#x27;a&#x27;)</span><br><span class="line">#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]</span><br></pre></td></tr></table></figure><p><strong>B 传正则表达式</strong></p><p>如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示和<strong>标签都应该被找到</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">for tag in soup.find_all(re.compile(&quot;^b&quot;)):</span><br><span class="line">    print(tag.name)</span><br><span class="line"># body</span><br><span class="line"># b</span><br></pre></td></tr></table></figure><p><strong>C 传列表</strong></p><p>如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有标签和<strong>标签:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all([&quot;a&quot;, &quot;b&quot;])</span><br><span class="line"># [&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;,</span><br><span class="line">#  &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,</span><br><span class="line">#  &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;,</span><br><span class="line">#  &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]</span><br></pre></td></tr></table></figure><p><strong>2）keyword 参数</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all(class = &quot;sister&quot;)</span><br><span class="line">#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]</span><br><span class="line"></span><br><span class="line">soup.find_all(id=&#x27;link2&#x27;)</span><br><span class="line"># [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;]</span><br></pre></td></tr></table></figure><p><strong>3）text 参数</strong></p><p>通过 text 参数可以搜索文档中的字符串内容，与 name 参数的可选值一样, text 参数接受 字符串 , 正则表达式 , 列表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all(text=&quot;Elsie&quot;)</span><br><span class="line"># [u&#x27;Elsie&#x27;]</span><br><span class="line"></span><br><span class="line">soup.find_all(text=[&quot;Tillie&quot;, &quot;Elsie&quot;, &quot;Lacie&quot;])</span><br><span class="line"># [u&#x27;Elsie&#x27;, u&#x27;Lacie&#x27;, u&#x27;Tillie&#x27;]</span><br><span class="line"></span><br><span class="line">soup.find_all(text=re.compile(&quot;Dormouse&quot;))</span><br><span class="line">[u&quot;The Dormouse&#x27;s story&quot;, u&quot;The Dormouse&#x27;s story&quot;]</span><br></pre></td></tr></table></figure><p><strong>find</strong></p><p>find的用法与find_all一样，区别在于find返回 第一个符合匹配结果，find_all则返回 所有匹配结果的列表</p><h3 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h3><p>这就是另一种与 find_all 方法有异曲同工之妙的查找方法，也是返回所有匹配结果的列表。</p><ul><li>写 CSS 时，标签名不加任何修饰，类名前加.，id名前加#</li><li>在这里我们也可以利用类似的方法来筛选元素，用到的方法是 soup.select()，返回类型是 list</li></ul><p><strong>（1）通过标签选择器查找</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print soup.select(&#x27;title&#x27;) </span><br><span class="line">#[&lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;]</span><br><span class="line"></span><br><span class="line">print soup.select(&#x27;a&#x27;)</span><br><span class="line">#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]</span><br><span class="line"></span><br><span class="line">print soup.select(&#x27;b&#x27;)</span><br><span class="line">#[&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;]</span><br></pre></td></tr></table></figure><p><strong>（2）通过类选择器查找</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print soup.select(&#x27;.sister&#x27;)</span><br><span class="line">#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]</span><br></pre></td></tr></table></figure><p><strong>（3）通过 id 选择器查找</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print soup.select(&#x27;#link1&#x27;)</span><br><span class="line">#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;]</span><br></pre></td></tr></table></figure><p><strong>（4）层级选择器 查找</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print soup.select(&#x27;p #link1&#x27;)</span><br><span class="line">#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;]</span><br></pre></td></tr></table></figure><p><strong>（5）通过属性选择器查找</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print soup.select(&#x27;a[class=&quot;sister&quot;]&#x27;)</span><br><span class="line">#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]</span><br><span class="line"></span><br><span class="line">print soup.select(&#x27;a[href=&quot;http://example.com/elsie&quot;]&#x27;)</span><br><span class="line">#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;]</span><br></pre></td></tr></table></figure><p><strong>(6) 获取文本内容 get_text()</strong></p><p>以上的 select 方法返回的结果都是列表形式，可以遍历形式输出，然后用 get_text() 方法来获取它的内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html, &#x27;lxml&#x27;)</span><br><span class="line">print type(soup.select(&#x27;title&#x27;))</span><br><span class="line">print soup.select(&#x27;title&#x27;)[0].get_text()</span><br><span class="line"></span><br><span class="line">for title in soup.select(&#x27;title&#x27;):</span><br><span class="line">    print title.get_text()</span><br></pre></td></tr></table></figure><p><strong>(7) 获取属性 get(‘属性的名字’)</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html, &#x27;lxml&#x27;)</span><br><span class="line">print type(soup.select(&#x27;a&#x27;))</span><br><span class="line">print soup.select(&#x27;a&#x27;)[0].get(&#x27;href&#x27;)</span><br></pre></td></tr></table></figure><h3 id="小结-14"><a href="#小结-14" class="headerlink" title="小结"></a>小结</h3><ol><li>安装beautifulsoup4:<code>pip install beautifulsoup4</code></li><li>beautifulsoup导包: <code>from bs4 import BeautifulSoup</code></li><li>beautifulsoup转换类型: <code>BeautifulSoup(html)</code></li><li>find 方法返回一个解析完毕的对象</li><li>findall 方法返回的是解析列表list</li><li>select 方法返回的是解析列表list</li><li>获取属性的方法: <code>get(&#39;属性名字&#39;)</code></li><li>和获取文本的方法: <code>get_text()</code></li></ol>]]></content>
      
      
      <categories>
          
          <category> python爬虫入门 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
