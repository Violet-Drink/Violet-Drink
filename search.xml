<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>python爬虫入门</title>
      <link href="/2023/05/16/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"/>
      <url>/2023/05/16/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="爬虫的基础知识"><a href="#爬虫的基础知识" class="headerlink" title="爬虫的基础知识"></a>爬虫的基础知识</h1><h2 id="为什么要学习爬虫"><a href="#为什么要学习爬虫" class="headerlink" title="为什么要学习爬虫"></a>为什么要学习爬虫</h2><p>如今，人工智能，大数据离我们越来越近，很多公司在开展相关的业务，但是人工智能和大数据中有一个东西非常重要，那就是数据，但是数据从哪里来呢？</p><p>首先我们来看下面这个例子：</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E6%96%B0%E6%B5%AA%E6%8C%87%E6%95%B0.png"></p><p>这是微博的<a href="http://data.weibo.com/index">微指数</a>的一个截图，他把在微博上的用户的微博和评论中的关键词语做了提取，然后进行了统计，然后根据统计结果得出某个词语的流行趋势，之后进行了简单的展示</p><p>类似微指数的网站还有很多，比如百度指数，阿里指数，360指数等等，这些网站有非常大的用户量，他们能够获取自己用户的数据进行统计和分析</p><p>那么对于一些中小型的公司，没有如此大的用户量的时候，他们该怎么办呢？</p><h3 id="数据的来源"><a href="#数据的来源" class="headerlink" title="数据的来源"></a>数据的来源</h3><ul><li>去第三方的公司购买数据(比如企查查)</li><li>去免费的数据网站下载数据(比如国家统计局)</li><li>通过爬虫爬取数据</li><li>人工收集数据(比如问卷调查)</li></ul><p>在上面的来源中：人工的方式费时费力，免费的数据网站上的数据质量不佳，很多第三方的数据公司他们的数据来源往往也是爬虫获取的，所以获取数据最有效的途径就是通过爬虫爬取</p><h3 id="爬取到的数据用途"><a href="#爬取到的数据用途" class="headerlink" title="爬取到的数据用途"></a>爬取到的数据用途</h3><p><a href="http://news.baidu.com/">百度新闻</a>,一家并不是做新闻的公司，这个网站上的新闻数据从哪里来的呢？</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%99%BE%E5%BA%A6%E6%96%B0%E9%97%BB.png"></p><p>通过点击，我们可以发现，他的新闻数据都是其他网站上的，在百度新闻上仅仅做了展示</p><p>如果后续我们要做一个网站，天天新闻，是不是也可以这样做呢</p><p>那么同样的，我们后续想要做一个和<a href="http://music.163.com/#/discover/playlist">网易云音乐</a>类似的音乐网站，是不是也可以这样来做呢？</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90.png"></p><p>通过前面的列子，能够总结出，爬虫获取的数据的用途：</p><ul><li>进行在网页或者是app上进行展示</li><li>进行数据分析或者是机器学习相关的项目</li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li>数据的来源：<ul><li>去第三方的公司购买数据(比如企查查)</li><li>去免费的数据网站下载数据(比如国家统计局)</li><li>通过爬虫爬取数据</li><li>人工收集数据(比如问卷调查)</li></ul></li><li>爬虫的概念：模拟浏览器发送网络请求，接收请求响应</li></ol><h2 id="什么是爬虫"><a href="#什么是爬虫" class="headerlink" title="什么是爬虫"></a>什么是爬虫</h2><p>网络爬虫（又被称为网页蜘蛛，网络机器人）就是模拟浏览器发送网络请求，接收请求响应，一种按照一定的规则，自动地抓取互联网信息的程序。</p><p>原则上，只要是浏览器(客户端)能做的事情，爬虫都能够做</p><h3 id="爬虫的用途"><a href="#爬虫的用途" class="headerlink" title="爬虫的用途"></a>爬虫的用途</h3><ul><li>12306抢票</li><li>网站上的投票</li><li>收集数据</li><li>刷流量和秒杀</li><li>爬虫调研</li></ul><h3 id="爬虫的分类"><a href="#爬虫的分类" class="headerlink" title="爬虫的分类"></a>爬虫的分类</h3><p>根据被爬网站的数量的不同，我们把爬虫分为：</p><ul><li>通用爬虫 ：通常指搜索引擎的爬虫</li><li>聚焦爬虫 ：针对特定网站的爬虫</li></ul><h3 id="爬虫的流程"><a href="#爬虫的流程" class="headerlink" title="爬虫的流程"></a>爬虫的流程</h3><p>聚焦爬虫的流程图：</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E8%81%9A%E7%84%A6%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B%E5%9B%BE.png"></p><ul><li>确认目标的url（地址）</li><li>发送网络请求（模拟正常的用户），得到对应的响应数据</li><li>提取出特定的数据</li><li>将数据进行保存，可以保存到本地、入库（数据库）</li></ul><h3 id="robots协议"><a href="#robots协议" class="headerlink" title="robots协议"></a>robots协议</h3><p>Robots协议：网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取，但它仅仅是互联网中的一般约定</p><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><ol><li>爬虫分类：通用爬虫、聚焦爬虫</li><li>爬虫的流程：<ul><li>向起始url发送请求，并获取响应</li><li>对响应进行提取</li><li>如果提取url，则继续发送请求获取响应</li><li>如果提取数据，则将数据进行保存</li></ul></li><li>robots协议：无需遵守该协议</li></ol><h2 id="http和https的概念"><a href="#http和https的概念" class="headerlink" title="http和https的概念"></a>http和https的概念</h2><ul><li>HTTP<ul><li>超文本传输协议</li><li>默认端口号:80</li></ul></li><li>HTTPS<ul><li>HTTP + SSL(安全套接字层)，即带有安全套接字层的超本文传输协议</li><li>默认端口号：443</li></ul></li></ul><p>HTTPS比HTTP更安全，但是性能更低</p><h3 id="浏览器发送HTTP请求的过程"><a href="#浏览器发送HTTP请求的过程" class="headerlink" title="浏览器发送HTTP请求的过程"></a>浏览器发送HTTP请求的过程</h3><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/http%E5%8F%91%E9%80%81%E7%9A%84%E8%BF%87%E7%A8%8B.png"></p><h3 id="http请求的过程"><a href="#http请求的过程" class="headerlink" title="http请求的过程"></a>http请求的过程</h3><ol><li>浏览器先向地址栏中的url发起请求，并获取相应</li><li>在返回的响应内容（html）中，会带有css、js、图片等url地址，以及ajax代码，浏览器按照响应内容中的顺序依次发送其他的请求，并获取相应的响应</li><li>浏览器每获取一个响应就对展示出的结果进行添加（加载），js，css等内容会修改页面的内容，js也可以重新发送请求，获取响应</li><li>从获取第一个响应并在浏览器中展示，直到最终获取全部响应，并在展示的结果中添加内容或修改————这个过程叫做浏览器的<strong>渲染</strong></li></ol><p><strong>注意:</strong></p><p>但是在爬虫中，爬虫只会请求url地址，对应的拿到url地址对应的响应（该响应的内容可以是html，css，js，图片等）</p><p>浏览器渲染出来的页面和爬虫请求的页面很多时候并不一样</p><p><strong>所以在爬虫中，需要以url地址对应的响应为准来进行数据的提取</strong></p><h3 id="http请求的形式"><a href="#http请求的形式" class="headerlink" title="http请求的形式"></a>http请求的形式</h3><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/http%E7%9A%84%E8%AF%B7%E6%B1%82%E5%BD%A2%E5%BC%8F.png"></p><p>这个图大家见过很多次，那么在浏览器headers中，点击view source来具体观察其中的请求行，请求头部和请求数据是什么样子的</p><h3 id="HTTP常见请求头"><a href="#HTTP常见请求头" class="headerlink" title="HTTP常见请求头"></a>HTTP常见请求头</h3><ol><li>Host (主机和端口号)</li><li>Connection (链接类型)</li><li>Upgrade-Insecure-Requests (升级为HTTPS请求)</li><li>User-Agent (浏览器名称)</li><li>Accept (传输文件类型)</li><li>Referer (页面跳转处)</li><li>Accept-Encoding（文件编解码格式）</li><li>Cookie （Cookie）</li><li>x-requested-with :XMLHttpRequest (表示该请求是Ajax异步请求)</li></ol><h3 id="HTTP重要的响应头"><a href="#HTTP重要的响应头" class="headerlink" title="HTTP重要的响应头"></a>HTTP重要的响应头</h3><ol><li>Set-Cookie （对方服务器设置cookie到用户浏览器的缓存）</li></ol><h3 id="响应状态码-status-code"><a href="#响应状态码-status-code" class="headerlink" title="响应状态码(status code)"></a>响应状态码(status code)</h3><p>常见的状态码：</p><ul><li>200：成功</li><li>302：临时转移至新的url</li><li>307：临时转移至新的url</li><li>404：找不到该页面</li><li>500：服务器内部错误</li><li>503：服务不可用，一般是被反爬</li></ul><h3 id="如何查看客户端和服务端的交流过程"><a href="#如何查看客户端和服务端的交流过程" class="headerlink" title="如何查看客户端和服务端的交流过程"></a>如何查看客户端和服务端的交流过程</h3><p>这里以Google浏览器中百度首页为例进行介绍：</p><ol><li><p>鼠标右键点击网页，点击“检查”</p></li><li><p>找到Network点击，里面存有网络数据的信息</p></li><li><p>刷新页面，点击访问百度首页</p></li><li><p>发现在Network中有很多的数据组成了我们看到的网页（html、css、js、jpg）</p></li><li><p>寻找All里面就是全部的数据包</p></li><li><p>一般，整体的骨架数据包，就是第一个</p><p>①Headers：请求的信息request响应的信息response</p><p>②Preview：预览效果的样式（图片的缺少代表需要其他的数据包填充）</p><p>③Response：数据包的类型是html，里面就是html数据的源代码</p></li><li><p>General：整体的信息描述</p><p>①Request URL：此数据包的域名</p><p>②Request Method：请求方式GETPOST</p><p>③Status Code：状态码</p><p>④Remote Address：ip:端口号</p></li><li><p>Response Headers（了解即可）：响应头（响应信息）</p><p>服务器需要遵循这种规则协议，浏览器才能解析出来，并且展示</p></li><li><p>Request Headers<strong>（重点）</strong>：请求头（请求信息）</p><p>（浏览器、爬虫程序）向服务器发送请求，需要遵循http、https协议</p><p>①Accept：告诉服务器请求的客户端所能接收的响应类型</p><p>②Accept-Encoding：所支持的压缩算法类型</p><p>③Accept-Language：所使用的语言偏好，可用于响应内容本地化</p><p>④Cache-Control：指定在客户端和服务器之间缓存和重新验证缓存的行为</p><p>⑤Connection：指定了客户端与服务器之间的连接类型</p><p>⑥Cookie<strong>（重点）</strong>：用于在客户端和服务端之间传递和储存信息</p><p>一个完整的Cookie通常包括了以下信息：</p><ul><li>名称：唯一标识一个Cookie的名称</li><li>值：与名称相关联的信息</li><li>域名：设置Cookie生效的域名</li><li>路径：设置Cookie生效的路径</li><li>过期时间：设置Cookie的过期时间，过期后浏览器会自动删除Cookie</li><li>安全标识：表示浏览器只能通过HTTPS和SSL等安全协议加密传输该Cookie的信息，提高Cookie传输过程中的安全性。</li></ul><p>⑦Host：指定了被请求资源所在的主机名（域名或者IP地址）</p><p>⑧User-Agent：包含了发出请求的用户代理标识信息。通过User-Agent头，服务器可以识别客户端使用的操作系统、浏览器和版本等信息，从而可以向客户端返回合适的内容</p></li></ol><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><ol><li><p>记忆 http、https的概念和区别：</p><ul><li>http: 超本文传输协议</li><li>https: HTTP + SSL，即带有安全套接字层的超本文传输协议</li></ul></li><li><p>记忆 浏览器发送http请求的过程:</p><ul><li>浏览器先向地址栏中的url发起请求，并获取相应</li><li>在返回的响应内容（html）中，会带有css、js、图片等url地址，以及ajax代码，浏览器按照响应内容中的顺序依次发送其他的请求，并获取相应的响应</li><li>浏览器每获取一个响应就对展示出的结果进行添加（加载），js，css等内容会修改页面的内容，js也可以重新发送请求，获取响应</li><li>从获取第一个响应并在浏览器中展示，直到最终获取全部响应，并在展示的结果中添加内容或修改</li></ul></li><li><p>记忆 http请求头的形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GET /item/<span class="number">503</span>/<span class="number">1227315</span>?fr=aladdin HTTP/<span class="number">1.1</span></span><br><span class="line">Host: www.baidu.com</span><br><span class="line">......</span><br></pre></td></tr></table></figure></li><li><p>记忆 http响应头的形式 :</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HTTP/<span class="number">1.1</span> <span class="number">200</span> OK</span><br><span class="line">Connection: keep-alive</span><br><span class="line">......</span><br></pre></td></tr></table></figure></li><li><p>了解 http响应状态码</p><ul><li>200：成功</li><li>302：临时转移至新的url</li></ul></li></ol><h2 id="字符串相关复习"><a href="#字符串相关复习" class="headerlink" title="字符串相关复习"></a>字符串相关复习</h2><h3 id="字符和字符集"><a href="#字符和字符集" class="headerlink" title="字符和字符集"></a>字符和字符集</h3><p>字符(Character)是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等</p><p>字符集(Character set)是多个字符的集合</p><p>字符集包括：ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集等</p><p>ASCII编码是1个字节，而Unicode编码通常是2个字节。</p><p><strong>UTF-8是Unicode的实现方式</strong>之一，UTF-8是它是一种变长的编码方式，可以是1，2，3个字节</p><h3 id="python3中的字符串"><a href="#python3中的字符串" class="headerlink" title="python3中的字符串"></a>python3中的字符串</h3><p>python3中两种字符串类型：</p><ul><li>str : unicode的呈现形式</li><li>bytes :字节类型，互联网上数据的都是以二进制的方式(字节类型)传输的</li></ul><h3 id="str和bytes类型的互相转换"><a href="#str和bytes类型的互相转换" class="headerlink" title="str和bytes类型的互相转换"></a>str和bytes类型的互相转换</h3><ul><li>str 使用encode方法转化为 bytes</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">&#x27;abc&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(s))</span><br><span class="line"><span class="comment">#str编码变为bytes类型</span></span><br><span class="line">b = s.encode()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(b))</span><br></pre></td></tr></table></figure><ul><li>bytes 通过decode转化为 str</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b = <span class="string">b&#x27;abc&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(b))</span><br><span class="line"><span class="comment">#bytes类型解码称为str类型</span></span><br><span class="line">s = b.decode()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(s))</span><br></pre></td></tr></table></figure><p><strong>注意：编码方式解码方式必须一样，否则就会出现乱码</strong></p><h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><ol><li>str，bytes以及互相转换:<ul><li>str 使用encode方法转化为 bytes</li><li>bytes 通过decode转化为 str</li></ul></li><li>字符集编码类型（ASCII,unicode,UTF-8）:<ul><li>字符(Character)是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等</li><li>字符集(Character set)有字符构成，是多个字符的集合，如：ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集等</li></ul></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/1.1%E6%80%BB%E7%BB%93xmind.png"></p><h1 id="请求的发送方法"><a href="#请求的发送方法" class="headerlink" title="请求的发送方法"></a>请求的发送方法</h1><h2 id="requests模块的基本使用"><a href="#requests模块的基本使用" class="headerlink" title="requests模块的基本使用"></a>requests模块的基本使用</h2><p>​requests是一个常用的Python第三方库，用于发送HTTP请求，并处理HTTP响应。它可以发送GET、POST、PUT、DELETE等类型的HTTP请求，并支持Session会话和Cookie的自动管理，同时还支持HTTP代理、HTTPS证书验证和上传文件等常见操作。它的基本用法非常简单，只<strong>需要发送HTTP请求并获取响应</strong>，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure><h3 id="为什么要重点学习requests模块，而不是urllib"><a href="#为什么要重点学习requests模块，而不是urllib" class="headerlink" title="为什么要重点学习requests模块，而不是urllib"></a>为什么要重点学习requests模块，而不是urllib</h3><ul><li>requests的底层实现就是urllib</li><li>requests在python2 和python3中通用，方法完全一样</li><li>requests简单易用</li><li>Requests能够自动帮助我们解压(gzip压缩的等)响应内容</li></ul><h3 id="requests模块的安装"><a href="#requests模块的安装" class="headerlink" title="requests模块的安装"></a>requests模块的安装</h3><p>Windows中requests模块安装：</p><ul><li>win + r &gt;&gt; 输入cmd &gt;&gt; 回车输入：pip install requests&#x3D;&#x3D;2.24.0 -i <a href="https://pypi.doubanio.com/simple">https://pypi.doubanio.com/simple</a></li><li><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/requests%E6%A8%A1%E5%9D%97%E5%AE%89%E8%A3%85.png"></li></ul><p>Mac中requests模块安装：</p><ul><li>win + r &gt;&gt; 输入cmd &gt;&gt; 回车输入：pip3 install requests&#x3D;&#x3D;2.24.0 -i <a href="https://pypi.doubanio.com/simple">https://pypi.doubanio.com/simple</a></li><li>没有Mac电脑图片就不展示了</li></ul><h3 id="requests模块发送简单的get请求、获取响应"><a href="#requests模块发送简单的get请求、获取响应" class="headerlink" title="requests模块发送简单的get请求、获取响应"></a>requests模块发送简单的get请求、获取响应</h3><p>需求：通过requests向百度首页发送请求，获取百度首页的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment">#目标url</span></span><br><span class="line">url = <span class="string">&quot;https://www.baidu.com&quot;</span></span><br><span class="line"><span class="comment">#向目标url发送get请求</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="comment"># 打印响应内容</span></span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure><p><strong>response的常用属性：</strong></p><ul><li><code>response.text</code> 响应体 str类型</li><li><code>respones.content</code> 响应体 bytes类型</li><li><code>response.status_code</code> 响应状态码</li><li><code>response.request.headers</code> 响应对应的请求头</li><li><code>response.headers</code> 响应头</li><li><code>response.request.cookies</code> 响应对应请求的cookie</li><li><code>response.cookies</code> 响应的cookie（经过了set-cookie动作）</li></ul><p> <strong>response.text 和response.content的区别</strong></p><ul><li><p><code>response.text</code></p><ul><li>类型：str</li><li>解码类型： requests模块自动根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码</li><li>如何修改编码方式：<code>response.encoding=”gbk”</code></li></ul></li><li><p><code>response.content</code></p><ul><li>类型：bytes</li><li>解码类型： 没有指定</li><li>如何修改编码方式：<code>response.content.decode(“utf8”)</code></li></ul></li></ul><p>获取网页源码的通用方式：</p><ol><li><code>response.content.decode()</code></li><li><code>response.content.decode(&quot;GBK&quot;)</code></li><li><code>response.text</code></li></ol><p>以上三种方法从前往后尝试，能够100%的解决所有网页解码的问题</p><p>所以：更推荐使用<code>response.content.decode()</code>的方式获取响应的html页面</p><h3 id="小练习：把网络上的图片保存到本地"><a href="#小练习：把网络上的图片保存到本地" class="headerlink" title="小练习：把网络上的图片保存到本地"></a>小练习：把网络上的图片保存到本地</h3><blockquote><p>我们来把<a href="http://www.baidu.com的logo图片保存到本地/">www.baidu.com的logo图片保存到本地</a></p></blockquote><p><strong>思考：</strong></p><ul><li>以什么方式打开文件</li><li>保存什么格式的内容</li></ul><p><strong>分析：</strong></p><ul><li>图片的url：<a href="https://www.baidu.com/img/flexible/logo/pc/index.png">https://www.baidu.com/img/flexible/logo/pc/index.png</a></li><li>利用requests模块发送请求获取响应</li><li>以二进制写入的方式打开文件，并将response响应的二进制内容写入</li></ul><p><strong>代码实现：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment">#图片的url</span></span><br><span class="line">url_ = <span class="string">&#x27;https://www.baidu.com/img/flexible/logo/pc/index.png&#x27;</span></span><br><span class="line"><span class="comment">#响应本身就是一个图片，并且是二进制类型</span></span><br><span class="line">response = requests.get(url_)</span><br><span class="line"><span class="comment">#以二进制+写入的方式打开文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;baidu.png&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content)</span><br></pre></td></tr></table></figure><h3 id="发送带header的请求"><a href="#发送带header的请求" class="headerlink" title="发送带header的请求"></a>发送带header的请求</h3><blockquote><p>我们先写一个获取百度首页的代码</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># 目标url</span></span><br><span class="line">url_ = <span class="string">&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line"><span class="comment"># 发送请求</span></span><br><span class="line">response = requests.get(url_)</span><br><span class="line"><span class="comment"># 打印响应对应的请求</span></span><br><span class="line"><span class="comment"># print(response.content)</span></span><br><span class="line"><span class="comment"># 打印响应对应请求的请求头信息</span></span><br><span class="line"><span class="built_in">print</span>(response.request.headers)</span><br></pre></td></tr></table></figure><p><strong>1.思考</strong></p><p>对比浏览器上百度首页的网页源码和代码中的百度首页的源码，有什么不同？</p><p>代码中的百度首页的源码非常少，为什么？</p><p><strong>2.为什么请求需要带上header？</strong></p><p>模拟浏览器，欺骗服务器，获取和浏览器一致的内容</p><p><strong>3.header的形式：字典</strong></p><p>headers &#x3D; {“User-Agent”: “Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;113.0.0.0 Safari&#x2F;537.36”}</p><p> <strong>4.用法</strong></p><p>requests.get(url, headers&#x3D;headers)</p><p><strong>完整的代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># 目标url</span></span><br><span class="line">url_ = <span class="string">&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line"><span class="comment"># 进行伪装</span></span><br><span class="line">headers = &#123;<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&quot;</span>&#125;</span><br><span class="line"><span class="comment"># 在请求头中带上User-Agent，模拟浏览器发送请求</span></span><br><span class="line">response = requests.get(url_,headers=headers)</span><br><span class="line"><span class="comment"># 打印响应对应的请求</span></span><br><span class="line"><span class="comment"># print(response.content)</span></span><br><span class="line"><span class="comment"># 打印响应对应请求的头信息</span></span><br><span class="line"><span class="built_in">print</span>(response.request.headers)</span><br></pre></td></tr></table></figure><h3 id="发送带参数的请求"><a href="#发送带参数的请求" class="headerlink" title="发送带参数的请求"></a>发送带参数的请求</h3><blockquote><p>我们在使用百度搜索的时候经常发现url地址中会有一个 <code>?</code>，那么该问号后边的就是请求参数，又叫做查询字符串</p></blockquote><p><strong>1.什么叫做请求参数</strong></p><p>例1： <a href="http://www.webkaka.com/tutorial/server/2015/021013/">http://www.webkaka.com/tutorial/server/2015/021013/</a></p><p>例2：<a href="https://www.baidu.com/s?wd=python&a=c">https://www.baidu.com/s?wd=python&amp;a=c</a></p><p>例1中没有请求参数！例2中?后边的就是请求参数</p><p><strong>2.请求参数的形式：字典</strong></p><p>kw &#x3D; {‘wd’:’长城’}</p><p><strong>3.请求参数的用法</strong></p><p>requests.get(url,params&#x3D;kw)</p><p><strong>4.关于参数的注意点</strong></p><p>在url地址中，很多参数是没有用的，比如百度搜索的url地址，其中参数只有一个字段有用，其他的都可以删除，如果确定那些请求参数有用或者没用：挨个尝试，对应的！在后续的爬虫中，遇到很多参数的url地址，都可以尝试删除参数</p><p><strong>两种方式：发送带参数的请求</strong></p><blockquote><p>对<code>https://www.baidu.com/s?wd=python</code>发起请求可以使用<code>requests.get(url, params=kw)</code>的方式</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式一：利用params参数发送带参数的请求</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># 进行伪装</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 这是目标url</span></span><br><span class="line"><span class="comment"># url_ = &#x27;https://www.baidu.com/s?&#x27;wd=python&#x27;</span></span><br><span class="line"><span class="comment"># 最后有没有问好结果都一样</span></span><br><span class="line">url_ = <span class="string">&#x27;https://www.baidu.com/s?&#x27;</span></span><br><span class="line"><span class="comment"># 请求参数是一个字典，即wd=python</span></span><br><span class="line">kw = &#123;<span class="string">&#x27;wd&#x27;</span>:<span class="string">&#x27;python&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 带上请求参数发起请求，获取响应</span></span><br><span class="line">response = requests.get(url_,headers,params=kw)</span><br><span class="line"><span class="comment"># 当有多个请求参数时，requests接收的params参数为多个键值对的字典，比如&#x27;?wd=python&amp;a=c&#x27;--&gt;&#123;&#x27;wd&#x27;:&#x27;python&#x27;,&#x27;a&#x27;:&#x27;c&#x27;&#125;</span></span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br></pre></td></tr></table></figure><blockquote><p>也可以直接对<code>https://www.baidu.com/s?wd=python</code>完整的url直接发送请求，不使用params参数</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式二：直接发送带参数的url请求</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&#x27;</span>&#125;</span><br><span class="line">url_ = <span class="string">&#x27;https://www.baidu.com/s?wd=python&#x27;</span></span><br><span class="line"><span class="comment"># kw = &#123;&#x27;wd&#x27;:&#x27;python&#x27;&#125;</span></span><br><span class="line"><span class="comment">#url中包含了请求参数，所以此时无需params</span></span><br><span class="line">response = requests.get(url_,headers=headers)</span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br></pre></td></tr></table></figure><h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><ol><li>requests模块的介绍：能够帮助我们发起请求获取响应</li><li>requests的基本使用：<code>requests.get(url)</code></li><li>以及response常见的属性：<ul><li><code>response.text</code> 响应体 str类型</li><li><code>respones.content</code> 响应体 bytes类型</li><li><code>response.status_code</code> 响应状态码</li><li><code>response.request.headers</code> 响应对应的请求头</li><li><code>response.headers</code> 响应头</li><li><code>response.request._cookies</code> 响应对应请求的cookie</li><li><code>response.cookies</code> 响应的cookie（经过了set-cookie动作）</li></ul></li><li>掌握 requests.text和content的区别：text返回str类型，content返回bytes类型</li><li>掌握 解决网页的解码问题：<ul><li><code>response.content.decode()</code></li><li><code>response.content.decode(&quot;GBK&quot;)</code></li><li><code>response.text</code></li></ul></li><li>掌握 requests模块发送带headers的请求：<code>requests.get(url, headers=&#123;&#125;)</code></li><li>掌握 requests模块发送带参数的get请求：<code>requests.get(url, params=&#123;&#125;)</code></li></ol><h2 id="requests模块的深入使用"><a href="#requests模块的深入使用" class="headerlink" title="requests模块的深入使用"></a>requests模块的深入使用</h2><h3 id="使用requests发送POST请求"><a href="#使用requests发送POST请求" class="headerlink" title="使用requests发送POST请求"></a>使用requests发送POST请求</h3><blockquote><p>思考：哪些地方我们会用到POST请求？</p></blockquote><ol><li>登录注册（ POST 比 GET 更安全）</li><li>需要传输大文本内容的时候（ POST 请求对数据长度没有要求）</li></ol><p>需要传输大文本内容的时候（ POST 请求对数据长度没有要求）</p><h3 id="requests发送post请求语法"><a href="#requests发送post请求语法" class="headerlink" title="requests发送post请求语法"></a>requests发送post请求语法</h3><ul><li>用法：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = requests.post(<span class="string">&#x27;http://www.baidu.com/&#x27;</span>,data = data,headers=headers)</span><br></pre></td></tr></table></figure><ul><li>data 的形式：字典</li></ul><h3 id="POST请求练习"><a href="#POST请求练习" class="headerlink" title="POST请求练习"></a>POST请求练习</h3><blockquote><p>下面面我们通过手机版百度翻译的例子看看post请求如何使用：</p><p>地址：<a href="http://fanyi.baidu.com/">http://fanyi.baidu.com/</a></p></blockquote><p><strong>思路分析</strong></p><ol><li><p>抓包确定请求的url地址</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%911.png"></p></li><li><p>确定请求的参数</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%912.png"></p></li><li><p>确定返回数据的位置</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%913.png"></p></li><li><p>模拟浏览器获取数据</p></li></ol><p><strong>小结</strong></p><p>在模拟登陆等场景，经常需要发送post请求，直接使用<code>requests.post(url,data)</code>即可</p><h3 id="为什么要使用代理"><a href="#为什么要使用代理" class="headerlink" title="为什么要使用代理"></a>为什么要使用代理</h3><ol><li>让服务器以为不是同一个客户端在请求</li><li>防止我们的真实地址被泄露，防止被追究</li></ol><h3 id="理解使用代理的过程"><a href="#理解使用代理的过程" class="headerlink" title="理解使用代理的过程"></a>理解使用代理的过程</h3><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E7%9A%84%E8%BF%87%E7%A8%8B.png"></p><h3 id="理解正向代理和反向代理的区别"><a href="#理解正向代理和反向代理的区别" class="headerlink" title="理解正向代理和反向代理的区别"></a>理解正向代理和反向代理的区别</h3><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86%E5%92%8C%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E7%9A%84%E5%8C%BA%E5%88%AB.png"></p><p>通过上图可以看出：</p><ul><li>正向代理：对于浏览器知道服务器的真实地址，例如VPN</li><li>反向代理：浏览器不知道服务器的真实地址，例如nginx</li></ul><h3 id="代理的使用"><a href="#代理的使用" class="headerlink" title="代理的使用"></a>代理的使用</h3><ul><li><p>用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.get(<span class="string">&quot;http://www.baidu.com&quot;</span>,  proxies = proxies)</span><br></pre></td></tr></table></figure></li><li><p>proxies的形式：字典</p></li><li><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">proxies = &#123; </span><br><span class="line">    <span class="string">&quot;http&quot;</span>: <span class="string">&quot;http://12.34.56.79:9527&quot;</span>, </span><br><span class="line">    <span class="string">&quot;https&quot;</span>: <span class="string">&quot;https://12.34.56.79:9527&quot;</span>, </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="代理IP的分类"><a href="#代理IP的分类" class="headerlink" title="代理IP的分类"></a>代理IP的分类</h3><p>根据代理ip的匿名程度，代理IP可以分为下面四类：</p><ul><li>透明代理(Transparent Proxy)：透明代理虽然可以直接“隐藏”你的IP地址，但是还是可以查到你是谁。</li><li>匿名代理(Anonymous Proxy)：使用匿名代理，别人只能知道你用了代理，无法知道你是谁。</li><li>高匿代理(Elite proxy或High Anonymity Proxy)：高匿代理让别人根本无法发现你是在用代理，所以是最好的选择。</li></ul><p>在使用的使用，毫无疑问使用高匿代理效果最好</p><p>从请求使用的协议可以分为：</p><ul><li>http代理</li><li>https代理</li><li>socket代理等</li></ul><p>不同分类的代理，在使用的时候需要根据抓取网站的协议来选择</p><h3 id="代理IP使用的注意点"><a href="#代理IP使用的注意点" class="headerlink" title="代理IP使用的注意点"></a>代理IP使用的注意点</h3><ul><li><p>反反爬</p><p>使用代理ip是非常必要的一种<code>反反爬</code>的方式</p><p>但是即使使用了代理ip，对方服务器任然会有很多的方式来检测我们是否是一个爬虫，比如：</p><ul><li><p>一段时间内，检测IP访问的频率，访问太多频繁会屏蔽</p></li><li><p>检查Cookie，User-Agent，Referer等header参数，若没有则屏蔽</p></li><li><p>服务方购买所有代理提供商，加入到反爬虫数据库里，若检测是代理则屏蔽</p><p>所以更好的方式在使用代理ip的时候使用随机的方式进行选择使用，不要每次都用一个代理ip</p></li></ul></li><li><p>代理ip池的更新</p><p>购买的代理ip很多时候大部分(超过60%)可能都没办法使用，这个时候就需要通过程序去检测哪些可用，把不能用的删除掉。</p></li></ul><h3 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h3><ol><li>requests发送post请求使用requests.post方法，带上请求体，其中请求体需要时字典的形式，传递给data参数接收</li><li>在requests中使用代理，需要准备字典形式的代理，传递给proxies参数接收</li><li>不同协议的url地址，需要使用不同的代理去请求</li></ol><h2 id="requests模块处理cookie"><a href="#requests模块处理cookie" class="headerlink" title="requests模块处理cookie"></a>requests模块处理cookie</h2><h3 id="爬虫中使用cookie"><a href="#爬虫中使用cookie" class="headerlink" title="爬虫中使用cookie"></a>爬虫中使用cookie</h3><blockquote><p>为了能够通过爬虫获取到登录后的页面，或者是解决通过cookie的反扒，需要使用request来处理cookie相关的请求</p></blockquote><p><strong>爬虫中使用cookie的利弊</strong></p><ol><li>带上cookie的好处<ul><li>能够访问登录后的页面</li><li>能够实现部分反反爬</li></ul></li><li>带上cookie的坏处<ul><li>一套cookie往往对应的是一个用户的信息，请求太频繁有更大的可能性被对方识别为爬虫</li><li>那么上面的问题如何解决 ?使用多个账号</li></ul></li></ol><p><strong>requests处理cookie的方法</strong></p><p>使用requests处理cookie有三种方法：</p><ol><li>cookie字符串放在headers中</li><li>把cookie字典放传给请求方法的cookies参数接收</li><li>使用requests提供的session模块</li></ol><h3 id="cookie添加在heades中"><a href="#cookie添加在heades中" class="headerlink" title="cookie添加在heades中"></a>cookie添加在heades中</h3><p><strong>headers中cookie的位置</strong></p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/cookie.png"></p><ul><li>headers中的cookie：<ul><li>使用分号(;)隔开</li><li>分号两边的类似a&#x3D;b形式的表示一条cookie</li><li>a&#x3D;b中，a表示键（name），b表示值（value）</li><li>在headers中仅仅使用了cookie的name和value</li></ul></li></ul><p><strong>cookie的具体组成的字段</strong></p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/cookie%E5%AD%97%E6%AE%B5.png"></p><p>由于headers中对cookie仅仅使用它的name和value，所以在代码中我们仅仅需要cookie的name和value即可</p><p><strong>在headers中使用cookie</strong></p><p>复制浏览器中的cookie到代码中使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line"><span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&quot;</span>,</span><br><span class="line"><span class="string">&quot;Cookie&quot;</span>:<span class="string">&quot;BIDUPSID=A90C08D3E5A4CF715B19A45EF0C36277; PSTM=1681971547; BAIDUID=A90C08D3E5A4CF710048D190A8AFC834:FG=1; BD_UPN=12314753; ZFY=xt9EwObfUTgPelIsIlSVDjUJToSnzhC5qjcv1rmfSX4:C; BAIDUID_BFESS=A90C08D3E5A4CF710048D190A8AFC834:FG=1; COOKIE_SESSION=7173_1_7_9_8_6_1_0_7_4_0_0_7174_0_6_0_1684733694_1684233838_1684733688%7C9%231930841_3_1684233835%7C2; baikeVisitId=24f4253b-72b8-4f90-9510-1eba650828fe; BA_HECTOR=0581a1a0ag85a12h8h2481e51i6roc81n; BD_HOME=1; H_PS_PSSID=38516_36550_38540_38614_38576_38486_38196_38636_26350&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">requests.get(url,headers=headers)</span><br></pre></td></tr></table></figure><p><strong>注意：</strong></p><p>cookie有过期时间 ，所以直接复制浏览器中的cookie可能意味着下一程序继续运行的时候需要替换代码中的cookie，对应的我们也可以通过一个程序专门来获取cookie供其他程序使用；当然也有很多网站的cookie过期时间很长，这种情况下，直接复制cookie来使用更加简单</p><h3 id="使用cookies参数接收字典形式的cookie"><a href="#使用cookies参数接收字典形式的cookie" class="headerlink" title="使用cookies参数接收字典形式的cookie"></a>使用cookies参数接收字典形式的cookie</h3><ul><li>cookies的形式：字典</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cookies = <span class="punctuation">&#123;</span><span class="attr">&quot;cookie的name&quot;</span><span class="punctuation">:</span><span class="string">&quot;cookie的value&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><ul><li>使用方法：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.get(url,headers=headers,cookies=cookie_dict&#125;</span><br></pre></td></tr></table></figure><h3 id="使用requests-session处理cookie"><a href="#使用requests-session处理cookie" class="headerlink" title="使用requests.session处理cookie"></a>使用requests.session处理cookie</h3><blockquote><p>前面使用手动的方式使用cookie，那么有没有更好的方法在requets中处理cookie呢？</p></blockquote><p>requests 提供了一个叫做session类，来实现客户端和服务端的<code>会话保持</code></p><p>会话保持有两个内涵：</p><ul><li>保存cookie，下一次请求会带上前一次的cookie</li><li>实现和服务端的长连接，加快请求速度</li></ul><p><strong>使用方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session = requests.session()</span><br><span class="line">response = session.get(url,headers)</span><br></pre></td></tr></table></figure><p>session实例在请求了一个网站后，对方服务器设置在本地的cookie会保存在session中，下一次再使用session请求对方服务器的时候，会带上前一次的cookie</p><p><strong>动手练习：</strong></p><p>动手尝试使用session来登录人人网： <a href="http://www.renren.com/PLogin.do">http://www.renren.com/PLogin.do</a> (先不考虑这个url地址从何而来)，请求体的格式：<code>&#123;&quot;email&quot;:&quot;username&quot;, &quot;password&quot;:&quot;password&quot;&#125;</code></p><p><strong>思路分析</strong></p><ol><li>准备url地址和请求参数</li><li>构造session发送post请求</li><li>使用session请求个人主页，观察是否请求成功</li></ol><h3 id="小结-6"><a href="#小结-6" class="headerlink" title="小结"></a>小结</h3><ol><li>cookie字符串可以放在headers字典中，键为Cookie，值为cookie字符串</li><li>可以把cookie字符串转化为字典，使用请求方法的cookies参数接收</li><li>使用requests提供的session模块，能够自动实现cookie的处理，包括请求的时候携带cookie，获取响应的时候保存cookie</li></ol><h2 id="requests的其他方法"><a href="#requests的其他方法" class="headerlink" title="requests的其他方法"></a>requests的其他方法</h2><h3 id="requests中cookirJar的处理方法"><a href="#requests中cookirJar的处理方法" class="headerlink" title="requests中cookirJar的处理方法"></a>requests中cookirJar的处理方法</h3><blockquote><p>使用request获取的resposne对象，具有cookies属性，能够获取对方服务器设置在本地的cookie，但是如何使用这些cookie呢？</p></blockquote><p><strong>方法介绍</strong></p><ol><li>response.cookies是CookieJar类型</li><li>使用requests.utils.dict_from_cookiejar，能够实现把cookiejar对象转化为字典</li></ol><p><strong>方法展示</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line"><span class="comment">#发送请求，获取response</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response.cookies))</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用方法从cookiejar中提取数据</span></span><br><span class="line">cookies = requests.utils.dict_from_cookiejar(response.cookies)</span><br><span class="line"><span class="built_in">print</span>(cookies)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;requests.cookies.RequestsCookieJar&#x27;</span>&gt;</span><br><span class="line">&#123;<span class="string">&#x27;BDORZ&#x27;</span>: <span class="string">&#x27;27315&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong></p><p>在前面的requests的session类中，我们不需要处理cookie的任何细节，如果有需要，我们可以使用上述方法来解决</p><h3 id="requests处理证书错误"><a href="#requests处理证书错误" class="headerlink" title="requests处理证书错误"></a>requests处理证书错误</h3><blockquote><p>经常我们在网上冲浪时，经常能够看到下面的提示：</p></blockquote><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/12306ssl%E9%94%99%E8%AF%AF.png"></p><p>出现这个问题的原因是：ssl的证书不安全导致</p><p><strong>代码中发起请求的效果</strong></p><p>那么如果在代码中请求会怎么样呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.12306.cn/mormhweb/&quot;</span></span><br><span class="line">response = requests.get(url)</span><br></pre></td></tr></table></figure><p>返回证书错误，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssl.CertificateError ...</span><br></pre></td></tr></table></figure><p><strong>解决方案</strong></p><p>为了在代码中能够正常的请求，我们修改添加一个参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.12306.cn/mormhweb/&quot;</span></span><br><span class="line">response = requests.get(url,verify=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="超时参数的使用"><a href="#超时参数的使用" class="headerlink" title="超时参数的使用"></a>超时参数的使用</h3><blockquote><p>在平时网上冲浪的过程中，我们经常会遇到网络波动，这个时候，一个请求等了很久可能任然没有结果</p><p>在爬虫中，一个请求很久没有结果，就会让整个项目的效率变得非常低，这个时候我们就需要对请求进行强制要求，让他必须在特定的时间内返回结果，否则就报错</p></blockquote><p><strong>超时参数使用方法如下：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = requests.get(url,timeout=3)</span><br></pre></td></tr></table></figure><p>通过添加timeout参数，能够保证在3秒钟内返回响应，否则会报错</p><p><strong>注意：</strong></p><p>这个方法还能够拿来检测代理ip的质量，如果一个代理ip在很长时间没有响应，那么添加超时之后也会报错，对应的这个ip就可以从代理ip池中删除</p><h3 id="retrying模块的使用"><a href="#retrying模块的使用" class="headerlink" title="retrying模块的使用"></a>retrying模块的使用</h3><blockquote><p>使用超时参数能够加快我们整体的请求速度，但是在正常的网页浏览过成功，如果发生速度很慢的情况，我们会做的选择是<strong>刷新页面</strong>，那么在代码中，我们是否也可以刷新请求呢？</p></blockquote><p>对应的，retrying模块就可以帮助我们解决</p><p><strong>retrying模块的使用</strong></p><p>retrying模块的地址：<a href="https://pypi.org/project/retrying/">https://pypi.org/project/retrying/</a></p><p>retrying 模块的使用</p><ol><li>使用retrying模块提供的retry模块</li><li>通过装饰器的方式使用，让被装饰的函数反复执行</li><li>retry中可以传入参数<code>stop_max_attempt_number</code>,让函数报错后继续重新执行，达到最大执行次数的上限，如果每次都报错，整个函数报错，如果中间有一个成功，程序继续往后执行</li></ol><p><strong>retrying和requests的简单封装</strong></p><p>实现一个发送请求的函数，每次爬虫中直接调用该函数即可实现发送请求，在其中</p><ul><li>使用timeout实现超时报错</li><li>使用retrying模块实现重试</li></ul><p>代码参考:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> retrying <span class="keyword">import</span> retry</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Parse</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.url_ = <span class="string">&#x27;xxxxxx.com&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#最大重试3次，3次全部报错，才会报错</span></span><br><span class="line"><span class="meta">    @retry(<span class="params">stop_max_attempt_number=<span class="number">3</span></span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_parse_url</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#超时的时候回报错并重试</span></span><br><span class="line">        response = requests.get(self.url_,timeout=<span class="number">3</span>)</span><br><span class="line">        <span class="comment">#状态码不是200，也会报错并重试</span></span><br><span class="line">        <span class="keyword">assert</span> response.status_code == <span class="number">200</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_url</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">try</span>:<span class="comment">#进行异常捕获</span></span><br><span class="line">            response = self._parse_url()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line">            <span class="comment">#报错返回None</span></span><br><span class="line">            response = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parse_ = Parse()</span><br><span class="line">    parse_.parse_url()</span><br></pre></td></tr></table></figure><h3 id="小结-7"><a href="#小结-7" class="headerlink" title="小结"></a>小结</h3><ol><li>requests.utils.dict_from_cookiejar能够实现cookiejar转化为字典</li><li>请求方法中添加verify&#x3D;False能够实现请求过程中不验证证书</li><li>请求方法中添加timeout能够实现强制程序返回结果的能够，否则会报错</li><li>retrying模块能够实现捕获函数的异常，反复执行函数的效果，和timeout配合使用，能够解决网络波动带来的请求不成功的问题</li></ol><h2 id="urllib的学习"><a href="#urllib的学习" class="headerlink" title="urllib的学习"></a>urllib的学习</h2><h3 id="urllib介绍"><a href="#urllib介绍" class="headerlink" title="urllib介绍"></a>urllib介绍</h3><p>除了requests模块可以发送请求之外, urllib模块也可以实现请求的发送,只是操作方法略有不同!</p><p>urllib在python中分为urllib和urllib2，在python3中为urllib</p><p>下面以python3的urllib为例进行讲解</p><h3 id="urllib的基本方法介绍"><a href="#urllib的基本方法介绍" class="headerlink" title="urllib的基本方法介绍"></a>urllib的基本方法介绍</h3><p><strong>urllib.urlopoen</strong></p><ol><li><p>传入URL地址</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = urllib.urlopen(<span class="string">&quot;http://www.baidu.com&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p>传入request对象</p></li></ol><p><strong>urllib.Request</strong></p><ol><li><p>构造简单请求</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构造请求</span></span><br><span class="line">request = urllib.request.Request(<span class="string">&quot;http://www.baidu.com&quot;</span>)</span><br><span class="line"><span class="comment">#发送请求获取响应</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br></pre></td></tr></table></figure></li><li><p>传入headers参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构造headers</span></span><br><span class="line">headers = &#123;<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&quot;</span>&#125;</span><br><span class="line"><span class="comment">#构造请求</span></span><br><span class="line">request = urllib.request.Request(url,headers=headers)</span><br><span class="line"><span class="comment">#发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br></pre></td></tr></table></figure></li><li><p>传入data参数 实现发送post请求</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构造headers</span></span><br><span class="line">headers = &#123;<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36&quot;</span>&#125;</span><br><span class="line"><span class="comment">#构造请求体</span></span><br><span class="line">formdata = &#123;</span><br><span class="line"><span class="string">&quot;type&quot;</span>:<span class="string">&quot;AUTO&quot;</span>,</span><br><span class="line"><span class="string">&quot;i&quot;</span>:<span class="string">&quot;i love python&quot;</span>,</span><br><span class="line"><span class="string">&quot;doctype&quot;</span>:<span class="string">&quot;json&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#构造请求</span></span><br><span class="line">request = urllib.request.Request(url,data=formdata,headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="built_in">print</span>(response.read())</span><br></pre></td></tr></table></figure></li></ol><p><strong>response.read()</strong></p><p>获取响应的html字符串,bytes类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#发送请求</span></span><br><span class="line">response = urllib.urlopen(<span class="string">&quot;http://www.baidu.com&quot;</span>)</span><br><span class="line"><span class="comment">#获取响应</span></span><br><span class="line">response.read()</span><br></pre></td></tr></table></figure><p><strong>urllib请求百度首页的完整例子</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line"><span class="comment">#构造headers</span></span><br><span class="line">headers = &#123;<span class="string">&quot;User-Agent&quot;</span> : <span class="string">&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)&quot;</span>&#125;</span><br><span class="line"><span class="comment">#构造请求</span></span><br><span class="line">request = urllib.request.Request(url, headers = headers)</span><br><span class="line"><span class="comment">#发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="comment">#获取html字符串</span></span><br><span class="line">html_str = response.read().decode()</span><br><span class="line"><span class="built_in">print</span>(html_str)</span><br></pre></td></tr></table></figure><h3 id="小结-8"><a href="#小结-8" class="headerlink" title="小结"></a>小结</h3><ol><li>urllib.request中实现了构造请求和发送请求的方法</li><li>urllib.request.Request(url,headers,data)能够构造请求</li><li>urllib.request.urlopen能够接受request请求或者url地址发送请求，获取响应</li><li>response.read()能够实现获取响应中的bytes字符串</li></ol><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/1.2%E6%80%BB%E7%BB%93xmind.png"></p><h1 id="数据提取方法"><a href="#数据提取方法" class="headerlink" title="数据提取方法"></a>数据提取方法</h1><h2 id="数据提取的概念"><a href="#数据提取的概念" class="headerlink" title="数据提取的概念"></a>数据提取的概念</h2><p>Python爬虫通过HTTP请求获取网页内容，然后使用各种技术（如正则表达式、beautifulsoup、pyquery等）从HTML中提取数据。</p><p>数据提取的目的是将网页中的有用信息提取出来。例如，在爬取电商网站时，可以提取商品名称、价格、描述、用户评价等信息。在社交媒体上爬取时，可以提取用户信息、关注数、转发数等信息。通过提取这些有用的数据，可以进行数据分析、数据挖掘、信息展示等操作。</p><h3 id="爬虫中数据的分类"><a href="#爬虫中数据的分类" class="headerlink" title="爬虫中数据的分类"></a>爬虫中数据的分类</h3><blockquote><p>在爬虫爬取的数据中有很多不同类型的数据,我们需要了解数据的不同类型来又规律的提取和解析数据.</p></blockquote><ul><li>结构化数据：json，xml等<ul><li>处理方式：直接转化为python类型</li></ul></li><li>非结构化数据：HTML<ul><li>处理方式：正则表达式、xpath</li></ul></li></ul><p>下面以今日头条的首页为例，介绍结构化数据和非结构化数据</p><ul><li><p>结构化数据例子：</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE.png"></p></li><li><p>非结构化数据：</p><p><img src="/img/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE.png"></p></li><li><p>XML数据：</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;bookstore&gt;</span><br><span class="line">&lt;book category=&quot;COOKING&quot;&gt;</span><br><span class="line">  &lt;title lang=&quot;en&quot;&gt;Everyday Italian&lt;/title&gt; </span><br><span class="line">  &lt;author&gt;Giada De Laurentiis&lt;/author&gt; </span><br><span class="line">  &lt;year&gt;2005&lt;/year&gt; </span><br><span class="line">  &lt;price&gt;30.00&lt;/price&gt; </span><br><span class="line">&lt;/book&gt;</span><br><span class="line">&lt;book category=&quot;CHILDREN&quot;&gt;</span><br><span class="line">  &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt; </span><br><span class="line">  &lt;author&gt;J K. Rowling&lt;/author&gt; </span><br><span class="line">  &lt;year&gt;2005&lt;/year&gt; </span><br><span class="line">  &lt;price&gt;29.99&lt;/price&gt; </span><br><span class="line">&lt;/book&gt;</span><br><span class="line">&lt;book category=&quot;WEB&quot;&gt;</span><br><span class="line">  &lt;title lang=&quot;en&quot;&gt;Learning XML&lt;/title&gt; </span><br><span class="line">  &lt;author&gt;Erik T. Ray&lt;/author&gt; </span><br><span class="line">  &lt;year&gt;2003&lt;/year&gt; </span><br><span class="line">  &lt;price&gt;39.95&lt;/price&gt; </span><br><span class="line">&lt;/book&gt;</span><br><span class="line">&lt;/bookstore&gt;</span><br></pre></td></tr></table></figure><p>从上面可以看出，xml数据也是结构非常明显的</p><h3 id="小结-9"><a href="#小结-9" class="headerlink" title="小结"></a>小结</h3><ol><li>爬虫中数据分类之结构化数据: json,xml</li><li>爬虫中数据分类之非结构化数据:Html,字符串</li><li>结构化数据处理的方式有:jsonpath,xpath,转换python类型处理,bs4</li><li>非结构化数据处理方式有:正则表达式,xpath,bs4</li></ol>]]></content>
      
      
      <categories>
          
          <category> python爬虫入门 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
